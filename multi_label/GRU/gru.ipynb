{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362560"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_path = './phrase_model_data.pkl'\n",
    "with open(train_path, 'rb') as f:\n",
    "    traindata = pickle.load(f)\n",
    "len(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./e3_process_data.pkl', 'rb') as f:\n",
    "    process_data = pickle.load(f)\n",
    "    \n",
    "w2i = process_data['w2i']\n",
    "# i2w = process_data['i2w']\n",
    "phrase_rule_133 = process_data['sampled_phrase']\n",
    "i2phrase_rule_133 = {i: p for p, i in phrase_rule_133.items()}\n",
    "len(phrase_rule_133)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 128 # 256 may be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tempdata, phrase_rule_133, w2i, max_len=1000):\n",
    "    half_len = max_len // 2\n",
    "    lnis = []\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    num_classes = len(phrase_rule_133)\n",
    "    for lni, items in tempdata.items():\n",
    "        label = [0] * num_classes\n",
    "        phrase = [phrase_rule_133[p] for p in items['phrases'] if p in phrase_rule_133]\n",
    "        if not phrase:\n",
    "            continue\n",
    "\n",
    "        if len(items['pred_sents']) > 6:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'][:3] + items['pred_sents'][-3:])\n",
    "        else:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'])\n",
    "        sentid = [w2i.get(w, 1) for w in sent.split()]\n",
    "        if not sentid:\n",
    "            continue\n",
    "        if len(sentid) > max_len:\n",
    "            sentid = sentid[:half_len] + sentid[-half_len:]\n",
    "\n",
    "        inputs.append(sentid)\n",
    "\n",
    "        for i in set(phrase):\n",
    "            label[i] = 1\n",
    "        assert sum(label) > 0\n",
    "        labels.append(label[:])\n",
    "        lnis.append(lni)\n",
    "\n",
    "    print(len(labels), len(inputs), len(lnis))\n",
    "    return lnis, inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320635 320635 320635\n"
     ]
    }
   ],
   "source": [
    "_, inputs, labels = prepare_data(traindata, phrase_rule_133, w2i, max_len=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=320635, minmax=(1, 128), mean=78.48248943502736, variance=2591.396846363558, skewness=-0.3131667640799144, kurtosis=-1.6436714830091887)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "lens = [len(x) for x in inputs]\n",
    "stats.describe(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320635, 133)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272535 (272535, 133) (48100, 133)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(inputs, labels, test_size=0.150012, random_state=0)\n",
    "print(len(x_train), y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 206  235  490  115   98   19   62    4 3570   85 1107  871    6   27\n",
      "   12   25  687 2196    6 1543   21  380  250 1846  960   29    6   80\n",
      "  180  269 1406 1538   85   27   17   53    6   47  210   67  525   13\n",
      "  507  692  211    7    8  236  737   41   84    9   13 6262 1764   76\n",
      "    5   10  300  562   64  119 3957   14   91  672  148  556    5  151\n",
      " 2906   71 1341 1468 1075 3405 3376   64    4   69 3754   24   60  298\n",
      "   30    6  144    4  377   87 1538  147 1111  153    5 1585    2   58\n",
      "  134 1054   86  793   61 1335  137 2239   18  394  164  107  169 4163\n",
      "   29   51   12    7   19    4  362   27  148 1981  318   25   74  121\n",
      "   87  274   41  281  175    1  122]\n"
     ]
    }
   ],
   "source": [
    "print(y_val.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PhraseData(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        super(PhraseData, self).__init__()\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i]\n",
    "    \n",
    "def collate_func(seqs, pad_token=0):\n",
    "    seq_lens = [len(seq) for seq in seqs]\n",
    "    max_len = max(seq_lens)\n",
    "    seqs = [seq + [pad_token] * (max_len - len(seq)) for seq in seqs]\n",
    "    return torch.LongTensor(seqs), torch.LongTensor(seq_lens)    \n",
    "    \n",
    "def pair_collate_func(inps):\n",
    "    pairs = sorted(inps, key=lambda p: len(p[0]), reverse=True)\n",
    "    seqs, tgt = zip(*pairs)\n",
    "    seqs, seq_lens = collate_func(seqs)\n",
    "    return seqs, seq_lens, torch.FloatTensor(tgt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 512\n",
    "train_loader = DataLoader(PhraseData(x_train, y_train), \n",
    "                          num_workers=1, \n",
    "                          batch_size=batch_sz, \n",
    "                          collate_fn=pair_collate_func, \n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(PhraseData(x_val, y_val), \n",
    "                        num_workers=1, \n",
    "                        batch_size=100, \n",
    "                        collate_fn=pair_collate_func, \n",
    "                        shuffle=False,\n",
    "                        drop_last=True) # 48100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128]) torch.Size([100]) torch.Size([100, 133])\n"
     ]
    }
   ],
   "source": [
    "for batch in val_loader:\n",
    "    x1, x2, x3 = batch\n",
    "    print(x1.size(), x2.size(), x3.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, n_layers, num_classes, dropout, weights):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=False)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size, n_layers, bidirectional=True, batch_first=True)\n",
    "        self.maxpooling = nn.AdaptiveMaxPool1d(1)\n",
    "        # self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_var, input_len):\n",
    "        embeded = self.emb(input_var) # b x l x emb_dim\n",
    "        embeded = self.dropout1(embeded)\n",
    "        \n",
    "        total_length = embeded.size(1)\n",
    "        \n",
    "        packed1 = torch.nn.utils.rnn.pack_padded_sequence(embeded, input_len, batch_first=True)\n",
    "        self.gru.flatten_parameters()\n",
    "        rnn1, hidden1 = self.gru(packed1)\n",
    "        rnn1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn1, batch_first=True, total_length=total_length) # b x l x 2hs\n",
    "\n",
    "        rnn1 = rnn1.permute(0, 2, 1) # b x 2hs x l\n",
    "        out = self.maxpooling(rnn1).squeeze(2) # b x 2hs x 1 -> b x 2hs\n",
    "        # out = self.dropout2(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# GRU\n",
    "def train_epoch(model, device, epoch, train_loader, test_loader, criterion, optimizer, clip=5.):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    t0 = time.time()\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        seqs, seq_lens, tgts = batch\n",
    "        seqs = seqs.to(device)\n",
    "        tgts = tgts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs, seq_lens)\n",
    "        loss = criterion(outputs, tgts)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            # print loss info every 20 Iterations\n",
    "            log_str = \"Epoch : {} , Iteration : {} , Time : {:.2f} , TrainLoss : {:.4f}\".format \\\n",
    "                        (epoch, i, time.time()-t0, train_loss/i)\n",
    "            print(log_str)\n",
    "            t0 = time.time()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            seqs, seq_lens, tgts = batch\n",
    "            seqs = seqs.to(device)\n",
    "            tgts = tgts.to(device)\n",
    "\n",
    "            outputs = model(seqs, seq_lens)\n",
    "            loss = criterion(outputs, tgts)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "    eval_loss /= len(val_loader)\n",
    "    return model, optimizer, train_loss, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiGRU(\n",
       "  (emb): Embedding(14456, 300)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (gru): GRU(300, 256, batch_first=True, bidirectional=True)\n",
       "  (maxpooling): AdaptiveMaxPool1d(output_size=1)\n",
       "  (linear): Linear(in_features=512, out_features=133, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 300\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "num_classes = 133\n",
    "dropout = 0.5\n",
    "emb_weights = np.load('./e3_EmbeddingMatrix.npy')\n",
    "bi_gru = BiGRU(emb_dim, hidden_size, n_layers, num_classes, dropout, emb_weights)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "bi_gru.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(bi_gru.parameters())\n",
    "optimizer = optim.Adam(bi_gru.parameters())\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Iteration : 100 , Time : 17.36 , TrainLoss : 0.0513\n",
      "Epoch : 1 , Iteration : 200 , Time : 17.12 , TrainLoss : 0.0495\n",
      "Epoch : 1 , Iteration : 300 , Time : 17.47 , TrainLoss : 0.0470\n",
      "Epoch : 1 , Iteration : 400 , Time : 17.55 , TrainLoss : 0.0446\n",
      "Epoch : 1 , Iteration : 500 , Time : 17.61 , TrainLoss : 0.0426\n",
      ">> Epoch : 1 , TrainLoss : 0.0420 , EvalLoss : 0.0322\n",
      "\n",
      "Epoch : 2 , Iteration : 100 , Time : 17.56 , TrainLoss : 0.0313\n",
      "Epoch : 2 , Iteration : 200 , Time : 17.55 , TrainLoss : 0.0305\n",
      "Epoch : 2 , Iteration : 300 , Time : 17.32 , TrainLoss : 0.0299\n",
      "Epoch : 2 , Iteration : 400 , Time : 17.42 , TrainLoss : 0.0293\n",
      "Epoch : 2 , Iteration : 500 , Time : 17.56 , TrainLoss : 0.0288\n",
      ">> Epoch : 2 , TrainLoss : 0.0287 , EvalLoss : 0.0258\n",
      "\n",
      "Epoch : 3 , Iteration : 100 , Time : 17.70 , TrainLoss : 0.0256\n",
      "Epoch : 3 , Iteration : 200 , Time : 17.31 , TrainLoss : 0.0253\n",
      "Epoch : 3 , Iteration : 300 , Time : 17.62 , TrainLoss : 0.0250\n",
      "Epoch : 3 , Iteration : 400 , Time : 17.54 , TrainLoss : 0.0247\n",
      "Epoch : 3 , Iteration : 500 , Time : 17.58 , TrainLoss : 0.0244\n",
      ">> Epoch : 3 , TrainLoss : 0.0244 , EvalLoss : 0.0229\n",
      "\n",
      "Epoch : 4 , Iteration : 100 , Time : 17.75 , TrainLoss : 0.0229\n",
      "Epoch : 4 , Iteration : 200 , Time : 17.57 , TrainLoss : 0.0227\n",
      "Epoch : 4 , Iteration : 300 , Time : 17.56 , TrainLoss : 0.0224\n",
      "Epoch : 4 , Iteration : 400 , Time : 17.61 , TrainLoss : 0.0223\n",
      "Epoch : 4 , Iteration : 500 , Time : 17.47 , TrainLoss : 0.0221\n",
      ">> Epoch : 4 , TrainLoss : 0.0221 , EvalLoss : 0.0212\n",
      "\n",
      "Epoch : 5 , Iteration : 100 , Time : 17.47 , TrainLoss : 0.0212\n",
      "Epoch : 5 , Iteration : 200 , Time : 17.40 , TrainLoss : 0.0210\n",
      "Epoch : 5 , Iteration : 300 , Time : 17.28 , TrainLoss : 0.0209\n",
      "Epoch : 5 , Iteration : 400 , Time : 17.25 , TrainLoss : 0.0208\n",
      "Epoch : 5 , Iteration : 500 , Time : 17.26 , TrainLoss : 0.0208\n",
      ">> Epoch : 5 , TrainLoss : 0.0207 , EvalLoss : 0.0205\n",
      "\n",
      "Epoch : 6 , Iteration : 100 , Time : 17.36 , TrainLoss : 0.0200\n",
      "Epoch : 6 , Iteration : 200 , Time : 17.29 , TrainLoss : 0.0200\n",
      "Epoch : 6 , Iteration : 300 , Time : 17.31 , TrainLoss : 0.0200\n",
      "Epoch : 6 , Iteration : 400 , Time : 17.55 , TrainLoss : 0.0199\n",
      "Epoch : 6 , Iteration : 500 , Time : 17.59 , TrainLoss : 0.0198\n",
      ">> Epoch : 6 , TrainLoss : 0.0199 , EvalLoss : 0.0198\n",
      "\n",
      "Epoch : 7 , Iteration : 100 , Time : 17.52 , TrainLoss : 0.0193\n",
      "Epoch : 7 , Iteration : 200 , Time : 17.50 , TrainLoss : 0.0193\n",
      "Epoch : 7 , Iteration : 300 , Time : 17.60 , TrainLoss : 0.0193\n",
      "Epoch : 7 , Iteration : 400 , Time : 17.52 , TrainLoss : 0.0192\n",
      "Epoch : 7 , Iteration : 500 , Time : 17.53 , TrainLoss : 0.0192\n",
      ">> Epoch : 7 , TrainLoss : 0.0192 , EvalLoss : 0.0194\n",
      "\n",
      "Epoch : 8 , Iteration : 100 , Time : 17.78 , TrainLoss : 0.0187\n",
      "Epoch : 8 , Iteration : 200 , Time : 17.60 , TrainLoss : 0.0187\n",
      "Epoch : 8 , Iteration : 300 , Time : 17.43 , TrainLoss : 0.0188\n",
      "Epoch : 8 , Iteration : 400 , Time : 17.38 , TrainLoss : 0.0187\n",
      "Epoch : 8 , Iteration : 500 , Time : 17.43 , TrainLoss : 0.0187\n",
      ">> Epoch : 8 , TrainLoss : 0.0187 , EvalLoss : 0.0190\n",
      "\n",
      "Epoch : 9 , Iteration : 100 , Time : 17.39 , TrainLoss : 0.0180\n",
      "Epoch : 9 , Iteration : 200 , Time : 17.45 , TrainLoss : 0.0183\n",
      "Epoch : 9 , Iteration : 300 , Time : 17.40 , TrainLoss : 0.0183\n",
      "Epoch : 9 , Iteration : 400 , Time : 17.51 , TrainLoss : 0.0182\n",
      "Epoch : 9 , Iteration : 500 , Time : 17.56 , TrainLoss : 0.0183\n",
      ">> Epoch : 9 , TrainLoss : 0.0183 , EvalLoss : 0.0189\n",
      "\n",
      "Epoch : 10 , Iteration : 100 , Time : 17.50 , TrainLoss : 0.0180\n",
      "Epoch : 10 , Iteration : 200 , Time : 17.54 , TrainLoss : 0.0180\n",
      "Epoch : 10 , Iteration : 300 , Time : 17.42 , TrainLoss : 0.0179\n",
      "Epoch : 10 , Iteration : 400 , Time : 17.36 , TrainLoss : 0.0179\n",
      "Epoch : 10 , Iteration : 500 , Time : 17.35 , TrainLoss : 0.0179\n",
      ">> Epoch : 10 , TrainLoss : 0.0179 , EvalLoss : 0.0189\n",
      "\n",
      "Epoch : 11 , Iteration : 100 , Time : 17.52 , TrainLoss : 0.0175\n",
      "Epoch : 11 , Iteration : 200 , Time : 17.37 , TrainLoss : 0.0177\n",
      "Epoch : 11 , Iteration : 300 , Time : 17.29 , TrainLoss : 0.0176\n",
      "Epoch : 11 , Iteration : 400 , Time : 17.31 , TrainLoss : 0.0176\n",
      "Epoch : 11 , Iteration : 500 , Time : 17.45 , TrainLoss : 0.0176\n",
      ">> Epoch : 11 , TrainLoss : 0.0176 , EvalLoss : 0.0186\n",
      "\n",
      "Epoch : 12 , Iteration : 100 , Time : 17.42 , TrainLoss : 0.0174\n",
      "Epoch : 12 , Iteration : 200 , Time : 17.31 , TrainLoss : 0.0173\n",
      "Epoch : 12 , Iteration : 300 , Time : 17.22 , TrainLoss : 0.0173\n",
      "Epoch : 12 , Iteration : 400 , Time : 17.23 , TrainLoss : 0.0173\n",
      "Epoch : 12 , Iteration : 500 , Time : 17.32 , TrainLoss : 0.0173\n",
      ">> Epoch : 12 , TrainLoss : 0.0173 , EvalLoss : 0.0185\n",
      "\n",
      "Epoch : 13 , Iteration : 100 , Time : 17.59 , TrainLoss : 0.0169\n",
      "Epoch : 13 , Iteration : 200 , Time : 17.55 , TrainLoss : 0.0170\n",
      "Epoch : 13 , Iteration : 300 , Time : 17.45 , TrainLoss : 0.0170\n",
      "Epoch : 13 , Iteration : 400 , Time : 17.51 , TrainLoss : 0.0170\n",
      "Epoch : 13 , Iteration : 500 , Time : 17.37 , TrainLoss : 0.0170\n",
      ">> Epoch : 13 , TrainLoss : 0.0170 , EvalLoss : 0.0183\n",
      "\n",
      "Epoch : 14 , Iteration : 100 , Time : 17.72 , TrainLoss : 0.0167\n",
      "Epoch : 14 , Iteration : 200 , Time : 17.62 , TrainLoss : 0.0167\n",
      "Epoch : 14 , Iteration : 300 , Time : 17.51 , TrainLoss : 0.0168\n",
      "Epoch : 14 , Iteration : 400 , Time : 17.58 , TrainLoss : 0.0168\n",
      "Epoch : 14 , Iteration : 500 , Time : 17.43 , TrainLoss : 0.0168\n",
      ">> Epoch : 14 , TrainLoss : 0.0168 , EvalLoss : 0.0184\n",
      "\n",
      "Epoch : 15 , Iteration : 100 , Time : 17.63 , TrainLoss : 0.0165\n",
      "Epoch : 15 , Iteration : 200 , Time : 17.30 , TrainLoss : 0.0166\n",
      "Epoch : 15 , Iteration : 300 , Time : 17.53 , TrainLoss : 0.0165\n",
      "Epoch : 15 , Iteration : 400 , Time : 17.54 , TrainLoss : 0.0165\n",
      "Epoch : 15 , Iteration : 500 , Time : 17.58 , TrainLoss : 0.0165\n",
      ">> Epoch : 15 , TrainLoss : 0.0165 , EvalLoss : 0.0183\n",
      "\n",
      "Epoch : 16 , Iteration : 100 , Time : 17.66 , TrainLoss : 0.0164\n",
      "Epoch : 16 , Iteration : 200 , Time : 17.52 , TrainLoss : 0.0164\n",
      "Epoch : 16 , Iteration : 300 , Time : 17.50 , TrainLoss : 0.0163\n",
      "Epoch : 16 , Iteration : 400 , Time : 17.52 , TrainLoss : 0.0163\n",
      "Epoch : 16 , Iteration : 500 , Time : 17.35 , TrainLoss : 0.0163\n",
      ">> Epoch : 16 , TrainLoss : 0.0163 , EvalLoss : 0.0182\n",
      "\n",
      "Epoch : 17 , Iteration : 100 , Time : 17.67 , TrainLoss : 0.0161\n",
      "Epoch : 17 , Iteration : 200 , Time : 17.42 , TrainLoss : 0.0161\n",
      "Epoch : 17 , Iteration : 300 , Time : 17.50 , TrainLoss : 0.0161\n",
      "Epoch : 17 , Iteration : 400 , Time : 17.28 , TrainLoss : 0.0161\n",
      "Epoch : 17 , Iteration : 500 , Time : 17.29 , TrainLoss : 0.0161\n",
      ">> Epoch : 17 , TrainLoss : 0.0161 , EvalLoss : 0.0181\n",
      "\n",
      "Epoch : 18 , Iteration : 100 , Time : 17.73 , TrainLoss : 0.0158\n",
      "Epoch : 18 , Iteration : 200 , Time : 17.59 , TrainLoss : 0.0158\n",
      "Epoch : 18 , Iteration : 300 , Time : 17.42 , TrainLoss : 0.0159\n",
      "Epoch : 18 , Iteration : 400 , Time : 17.50 , TrainLoss : 0.0159\n",
      "Epoch : 18 , Iteration : 500 , Time : 17.54 , TrainLoss : 0.0159\n",
      ">> Epoch : 18 , TrainLoss : 0.0159 , EvalLoss : 0.0181\n",
      "\n",
      "Epoch : 19 , Iteration : 100 , Time : 17.50 , TrainLoss : 0.0157\n",
      "Epoch : 19 , Iteration : 200 , Time : 17.40 , TrainLoss : 0.0157\n",
      "Epoch : 19 , Iteration : 300 , Time : 17.55 , TrainLoss : 0.0157\n",
      "Epoch : 19 , Iteration : 400 , Time : 17.62 , TrainLoss : 0.0157\n",
      "Epoch : 19 , Iteration : 500 , Time : 17.52 , TrainLoss : 0.0157\n",
      ">> Epoch : 19 , TrainLoss : 0.0157 , EvalLoss : 0.0181\n",
      "\n",
      "Epoch : 20 , Iteration : 100 , Time : 17.70 , TrainLoss : 0.0154\n",
      "Epoch : 20 , Iteration : 200 , Time : 17.45 , TrainLoss : 0.0155\n",
      "Epoch : 20 , Iteration : 300 , Time : 17.20 , TrainLoss : 0.0155\n",
      "Epoch : 20 , Iteration : 400 , Time : 17.48 , TrainLoss : 0.0155\n",
      "Epoch : 20 , Iteration : 500 , Time : 17.43 , TrainLoss : 0.0155\n",
      ">> Epoch : 20 , TrainLoss : 0.0155 , EvalLoss : 0.0182\n",
      "\n",
      "Epoch : 21 , Iteration : 100 , Time : 17.36 , TrainLoss : 0.0153\n",
      "Epoch : 21 , Iteration : 200 , Time : 17.33 , TrainLoss : 0.0153\n",
      "Epoch : 21 , Iteration : 300 , Time : 17.59 , TrainLoss : 0.0154\n",
      "Epoch : 21 , Iteration : 400 , Time : 17.72 , TrainLoss : 0.0154\n",
      "Epoch : 21 , Iteration : 500 , Time : 17.30 , TrainLoss : 0.0154\n",
      ">> Epoch : 21 , TrainLoss : 0.0154 , EvalLoss : 0.0179\n",
      "\n",
      "Epoch : 22 , Iteration : 100 , Time : 17.48 , TrainLoss : 0.0152\n",
      "Epoch : 22 , Iteration : 200 , Time : 17.17 , TrainLoss : 0.0151\n",
      "Epoch : 22 , Iteration : 300 , Time : 17.45 , TrainLoss : 0.0151\n",
      "Epoch : 22 , Iteration : 400 , Time : 17.66 , TrainLoss : 0.0152\n",
      "Epoch : 22 , Iteration : 500 , Time : 17.32 , TrainLoss : 0.0152\n",
      ">> Epoch : 22 , TrainLoss : 0.0152 , EvalLoss : 0.0181\n",
      "\n",
      "Epoch : 23 , Iteration : 100 , Time : 17.58 , TrainLoss : 0.0148\n",
      "Epoch : 23 , Iteration : 200 , Time : 17.43 , TrainLoss : 0.0149\n",
      "Epoch : 23 , Iteration : 300 , Time : 17.37 , TrainLoss : 0.0149\n",
      "Epoch : 23 , Iteration : 400 , Time : 17.33 , TrainLoss : 0.0150\n",
      "Epoch : 23 , Iteration : 500 , Time : 17.57 , TrainLoss : 0.0150\n",
      ">> Epoch : 23 , TrainLoss : 0.0150 , EvalLoss : 0.0180\n",
      "\n",
      "Epoch : 24 , Iteration : 100 , Time : 17.63 , TrainLoss : 0.0148\n",
      "Epoch : 24 , Iteration : 200 , Time : 17.57 , TrainLoss : 0.0148\n",
      "Epoch : 24 , Iteration : 300 , Time : 17.54 , TrainLoss : 0.0148\n",
      "Epoch : 24 , Iteration : 400 , Time : 17.81 , TrainLoss : 0.0148\n",
      "Epoch : 24 , Iteration : 500 , Time : 17.39 , TrainLoss : 0.0148\n",
      ">> Epoch : 24 , TrainLoss : 0.0148 , EvalLoss : 0.0181\n",
      "\n",
      "Epoch : 25 , Iteration : 100 , Time : 17.72 , TrainLoss : 0.0145\n",
      "Epoch : 25 , Iteration : 200 , Time : 17.60 , TrainLoss : 0.0145\n",
      "Epoch : 25 , Iteration : 300 , Time : 17.55 , TrainLoss : 0.0146\n",
      "Epoch : 25 , Iteration : 400 , Time : 17.66 , TrainLoss : 0.0146\n",
      "Epoch : 25 , Iteration : 500 , Time : 17.63 , TrainLoss : 0.0147\n",
      ">> Epoch : 25 , TrainLoss : 0.0147 , EvalLoss : 0.0180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "best_eval_loss = float('inf')\n",
    "save_path = './bi_gru_128.pt'\n",
    "for epoch in range(1, 1+n_epochs):\n",
    "    bi_gru, optimizer, train_loss, eval_loss = train_epoch(bi_gru, device, epoch, train_loader, val_loader, \n",
    "                                                           criterion, optimizer, clip=5.)\n",
    "\n",
    "    print(\">> Epoch : {} , TrainLoss : {:.4f} , EvalLoss : {:.4f}\\n\".format \\\n",
    "          (epoch, train_loss, eval_loss))\n",
    "\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        torch.save(bi_gru.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48100, 133]) torch.Size([48100, 133])\n"
     ]
    }
   ],
   "source": [
    "PATH = './bi_gru_128.pt'\n",
    "bi_gru.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def predict_res(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            seqs, seq_lens, tgts = batch\n",
    "            seqs = seqs.to(device)\n",
    "            tgts = tgts.to(device)\n",
    "            outputs = model(seqs, seq_lens)\n",
    "            \n",
    "            if y_true is None:\n",
    "                y_true = tgts\n",
    "            else:\n",
    "                y_true = torch.cat((y_true, tgts), 0)\n",
    "            \n",
    "            if y_pred is None:\n",
    "                y_pred = outputs\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred, outputs), 0)\n",
    "\n",
    "    print(y_true.size(), y_pred.size())\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    return y_true.cpu().numpy(), y_pred.cpu().numpy()\n",
    "\n",
    "val_true, val_pred = predict_res(bi_gru, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9683570006799356"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_val = roc_auc_score(val_true, val_pred)\n",
    "# 0.9696475056679279 - 256\n",
    "# 0.9678317424057077 - 512\n",
    "auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_active_value(s1, active_value):\n",
    "    # return np.round(s1)\n",
    "    return (s1 > active_value).astype(int)\n",
    "\n",
    "def cal_avg_p_r(arr_true, arr_pred, max_num=6, active_value=0.05):\n",
    "    ps, rs = [], []\n",
    "    for i in range(arr_true.shape[0]):\n",
    "        t1, s1 = arr_true[i], arr_pred[i]\n",
    "        if sum(t1) <= 0:\n",
    "            continue\n",
    "        s2 = fit_active_value(s1, active_value)\n",
    "        # if sum(s2) > max_num:\n",
    "        #     s2 = cut_max_num(s1, max_num)\n",
    "        # if sum(s2) > 4:\n",
    "        #     s2 = get_maxf_adj(s1, s2, adj_phrase_map, phrase_prob, cannot_be_only)\n",
    "        # s2 = fit_threshold(s1, thresholds)\n",
    "\n",
    "        p, r = precision_score(t1, s2), recall_score(t1, s2)\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "    return np.average(ps), np.average(rs), len(ps), ps, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3951539838274333, 0.9472644210144208, 48100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_p, avr_r, num, ps, rs = cal_avg_p_r(val_true, val_pred, active_value=0.02)\n",
    "# (0.4204987318031663, 0.9493316008316008, 48100) - 256\n",
    "# (0.4182258776883548, 0.9502048312048311, 48100) - 512\n",
    "avg_p, avr_r, num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112089"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = './phrase_model_data_ftc.pkl'\n",
    "with open(test_path, 'rb') as f:\n",
    "    testdata = pickle.load(f)\n",
    "len(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93581 93581 93581\n"
     ]
    }
   ],
   "source": [
    "_, test_inputs, test_labels = prepare_data(testdata, phrase_rule_133, w2i, max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93581, 133)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(PhraseData(test_inputs, test_labels), \n",
    "                        num_workers=1, \n",
    "                        batch_size=128, \n",
    "                        collate_fn=pair_collate_func, \n",
    "                        shuffle=False,\n",
    "                        drop_last=True) # 93581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93568, 133]) torch.Size([93568, 133])\n"
     ]
    }
   ],
   "source": [
    "test_true, test_pred = predict_res(bi_gru, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9819577986246634, 129)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_column_wise_auc(y_true, y_pred):\n",
    "    assert y_true.shape[1] == y_pred.shape[1],'Arrays must have the same dimension'\n",
    "    list_of_aucs = []\n",
    "    for column in range(y_true.shape[1]):\n",
    "        #print(sum(y_true[:,column]), sum(y_pred[:,column]))\n",
    "        if sum(y_true[:,column]) == 0:\n",
    "            continue\n",
    "        list_of_aucs.append(roc_auc_score(y_true[:,column],y_pred[:,column]))\n",
    "    # print(list_of_aucs)\n",
    "    return np.array(list_of_aucs).mean(), len((list_of_aucs))\n",
    "\n",
    "\n",
    "# (0.9820563869473041, 129) - 256\n",
    "# (0.9819577986246634, 129) - 512\n",
    "mean_column_wise_auc(test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44155424302135166, 0.966464254669891, 93568)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_p, avr_r, num, ps, rs = cal_avg_p_r(test_true, test_pred, active_value=0.02)\n",
    "# (0.4358161358395861, 0.9669629540285107, 93568) - 256\n",
    "# (0.44155424302135166, 0.966464254669891, 93568) - 512\n",
    "avg_p, avr_r, num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charley",
   "language": "python",
   "name": "charley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}