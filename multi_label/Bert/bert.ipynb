{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CUDA_VISIBLE_DEVICES=1\n",
    "import pickle\n",
    "import time\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0) \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertPreTrainedModel, AdamW, \\\n",
    "                         get_linear_schedule_with_warmup, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './phrase_model_data.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    traindata = pickle.load(f)\n",
    "len(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tempdata, phrase_rule_133):\n",
    "    lnis = []\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    num_classes = len(phrase_rule_133)\n",
    "    for lni, items in tempdata.items():\n",
    "        label = [0] * num_classes\n",
    "        phrase = [phrase_rule_133[p] for p in items['phrases'] if p in phrase_rule_133]\n",
    "        if not phrase:\n",
    "            continue\n",
    "\n",
    "        if len(items['pred_sents']) > 6:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'][:3] + items['pred_sents'][-3:])\n",
    "        else:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'])\n",
    "\n",
    "        if not sent:\n",
    "            continue\n",
    "\n",
    "        inputs.append(sent)\n",
    "\n",
    "        for i in set(phrase):\n",
    "            label[i] = 1\n",
    "        assert sum(label) > 0\n",
    "        labels.append(label[:])\n",
    "        lnis.append(lni)\n",
    "\n",
    "    print(len(labels), len(inputs), len(lnis))\n",
    "    return lnis, inputs, labels\n",
    "\n",
    "# _, inputs, labels = prepare_data(traindata, phrase_rule_133)\n",
    "# inputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93581 93581 93581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"extension of time granted to april , to file the reply to the informal response to the petition for writ of habeas corpus . extension is granted based upon deputy state public defender debra s . sabah press's representation that she anticipates filing that brief by .\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_path = './phrase_model_data_ftc.pkl'\n",
    "# with open(test_path, 'rb') as f:\n",
    "#     testdata = pickle.load(f)\n",
    "# print(len(testdata))\n",
    "\n",
    "test_lnis, test_inputs, test_labels = prepare_data(testdata, phrase_rule_133)\n",
    "test_inputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize_ids(inputs, tokenizer):\n",
    "    input_ids = [] # List[List[int]]\n",
    "\n",
    "    # For every sentence...\n",
    "    for i, sent in enumerate(inputs):\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                            # This function also supports truncation and conversion\n",
    "                            # to pytorch tensors, but we need to do padding, so we\n",
    "                            # can't use these features :( .\n",
    "                            # max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_sent)\n",
    "    # print(len(input_ids))\n",
    "    return input_ids\n",
    "\n",
    "test_input_ids = bert_tokenize_ids(test_inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93581"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 15339, 4267, 2038, 6406, 1037, 4367, 2005, 2019, 5331, 1997, 2051, 2000, 5371, 17304, 2000, 1996, 14351, 3648, 1005, 1055, 3818, 9556, 1998, 6749, 22137, 1012, 2008, 4367, 2003, 4379, 1012, 6254, 1012, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=320635, minmax=(3, 10479), mean=126.5668626319647, variance=12403.922152096984, skewness=3.800628086074086, kurtosis=238.0777072156434)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids[10])\n",
    "lengths = [len(x) for x in input_ids]\n",
    "stats.describe(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs : 93581\n",
      "Length of Attention mask : 93581\n"
     ]
    }
   ],
   "source": [
    "def prepare_attention_mask(new_inp_ids, pad_id):\n",
    "    assert pad_id == 0, 'bug of pad id'\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # For each sentence...\n",
    "    for sent in new_inp_ids:\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > pad_id) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "    print('Length of Attention mask : {}'.format(len(attention_masks)))\n",
    "    return attention_masks\n",
    "\n",
    "\n",
    "def truncate_pad_id(input_ids, tokenizer, max_len=256):\n",
    "    half_len = max_len // 2\n",
    "\n",
    "    # truncation and padding\n",
    "    pad_id = tokenizer.pad_token_id # 0\n",
    "\n",
    "    new_inp_ids = []\n",
    "    for x in input_ids:\n",
    "        ll = len(x)\n",
    "        if ll > max_len:\n",
    "            new_inp_ids.append(x[:half_len] + x[-half_len:])\n",
    "        elif ll < max_len:\n",
    "            new_inp_ids.append(x + [pad_id] * (max_len-ll))\n",
    "        else:\n",
    "            new_inp_ids.append(x)\n",
    "        assert len(new_inp_ids[-1]) == max_len\n",
    "\n",
    "    print('Length of inputs : {}'.format(len(new_inp_ids)))\n",
    "    attention_masks = prepare_attention_mask(new_inp_ids, pad_id)\n",
    "    return new_inp_ids, attention_masks\n",
    "\n",
    "new_test_inp_ids, test_attention_masks = truncate_pad_id(test_input_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11698"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_inputs = torch.tensor(new_test_inp_ids).long()\n",
    "# test_masks = torch.tensor(test_attention_masks).long()\n",
    "# test_labels = torch.tensor(test_labels).float()\n",
    "\n",
    "# test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=8)\n",
    "# len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in test_dataloader:\n",
    "    fi = b[0]\n",
    "    fl = b[2]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(new_inp_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.15)\n",
    "# Do the same for the masks.\n",
    "# train_masks, val_masks, _, _ = train_test_split(attention_masks, y,\n",
    "#                                              random_state=2020, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272539, 48096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same for the masks.\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=2020, test_size=0.15)\n",
    "len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_process_data = {'train_inputs': train_inputs, 'val_inputs': val_inputs, 'test_inputs': test_inputs, \n",
    "#                      'train_labels': train_labels, 'val_labels': val_labels, 'test_labels': test_labels, \n",
    "#                      'train_masks': train_masks, 'val_masks': val_masks, 'test_masks': test_masks, \n",
    "#                      'test_lnis': test_lnis}\n",
    "# with open('./e3_bert_process_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(bert_process_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "with open('./e3_bert_process_data.pkl', 'rb') as f:\n",
    "    bert_process_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# train_inputs = bert_process_data['train_inputs']\n",
    "# train_labels = bert_process_data['train_labels']\n",
    "# train_masks = bert_process_data['train_masks']\n",
    "\n",
    "val_inputs = bert_process_data['val_inputs']\n",
    "val_labels = bert_process_data['val_labels']\n",
    "val_masks = bert_process_data['val_masks']\n",
    "\n",
    "test_inputs, test_masks = bert_process_data['test_inputs'], bert_process_data['test_masks']\n",
    "test_labels, test_lnis = bert_process_data['test_labels'], bert_process_data['test_lnis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2717b7a784f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "train_inputs.size(), val_inputs.size(), test_inputs.size(), train_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5849"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# train_inputs = torch.tensor(train_inputs).long()\n",
    "# val_inputs = torch.tensor(val_inputs).long()\n",
    "\n",
    "# train_labels = torch.tensor(train_labels).float()\n",
    "# val_labels = torch.tensor(val_labels).float()\n",
    "\n",
    "# train_masks = torch.tensor(train_masks).long()\n",
    "# val_masks = torch.tensor(val_masks).long()\n",
    "\n",
    "# test_inputs = torch.tensor(new_test_inp_ids).long()\n",
    "# test_masks = torch.tensor(test_attention_masks).long()\n",
    "# test_labels = torch.tensor(test_labels).float()\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FtBert(nn.Module):\n",
    "#     def __init__(self, hidden_size=768, num_labels=133, dropout_prob=0.5):\n",
    "#         super(FtBert, self).__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased', \n",
    "#             cache_dir='/home/aidog/workspace/charley/sound/toxic_comment_c/bert')\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "#         self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs[1]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "#         return logits\n",
    "    \n",
    "\n",
    "class FtBert(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_labels=133, dropout_prob=0.5):\n",
    "        super(FtBert, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n"
     ]
    }
   ],
   "source": [
    "model = FtBert()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "PATH = './ft_bert_0224.pt'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "print('finished loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FtBert(BertPreTrainedModel):\n",
    "    def __init__(self, config, hidden_size=768, num_labels=133, dropout_prob=0.5):\n",
    "        super(FtBert, self).__init__(config)\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure = BertConfig()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # device = torch.device('cpu')\n",
    "# model = FtBert(configure)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, epoch, train_dataloader, validation_dataloader, \n",
    "                criterion, optimizer, clip=5.):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    t0 = time.time()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader, 1):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        # scheduler.step()\n",
    "        if step % 1000 == 0:\n",
    "            # print loss info every 20 Iterations\n",
    "            log_str = \"Epoch : {} , Iteration : {} , Time : {:.2f} , TrainLoss : {:.4f}\".format \\\n",
    "                        (epoch, step, time.time()-t0, train_loss/step)\n",
    "            print(log_str)\n",
    "            t0 = time.time()\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(validation_dataloader, 1):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            loss = criterion(outputs, b_labels)\n",
    "            eval_loss += loss.item()\n",
    "        eval_loss /= len(validation_dataloader)\n",
    "\n",
    "    return model, optimizer, train_loss, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Iteration : 1000 , Time : 324.37 , TrainLoss : 0.0001\n",
      "Epoch : 1 , Iteration : 2000 , Time : 324.54 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 3000 , Time : 326.64 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 4000 , Time : 327.24 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 5000 , Time : 326.42 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 6000 , Time : 326.00 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 7000 , Time : 326.05 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 8000 , Time : 326.23 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 9000 , Time : 326.26 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 10000 , Time : 326.65 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 11000 , Time : 326.82 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 12000 , Time : 326.87 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 13000 , Time : 326.97 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 14000 , Time : 325.77 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 15000 , Time : 327.17 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 16000 , Time : 323.40 , TrainLoss : 0.0000\n",
      "Epoch : 1 , Iteration : 17000 , Time : 327.13 , TrainLoss : 0.0000\n",
      ">> Epoch : 1 , TrainLoss : 0.0000 , EvalLoss : 0.0179\n",
      "\n",
      "Epoch : 2 , Iteration : 1000 , Time : 326.32 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 2000 , Time : 325.85 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 3000 , Time : 326.35 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 4000 , Time : 326.41 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 5000 , Time : 325.38 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 6000 , Time : 321.33 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 7000 , Time : 321.90 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 8000 , Time : 325.21 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 9000 , Time : 326.60 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 10000 , Time : 326.05 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 11000 , Time : 324.46 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 12000 , Time : 323.89 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 13000 , Time : 326.74 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 14000 , Time : 324.96 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 15000 , Time : 321.89 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 16000 , Time : 325.46 , TrainLoss : 0.0000\n",
      "Epoch : 2 , Iteration : 17000 , Time : 326.06 , TrainLoss : 0.0000\n",
      ">> Epoch : 2 , TrainLoss : 0.0000 , EvalLoss : 0.0161\n",
      "\n",
      "Epoch : 3 , Iteration : 1000 , Time : 322.13 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 2000 , Time : 324.01 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 3000 , Time : 325.49 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 4000 , Time : 323.71 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 5000 , Time : 323.42 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 6000 , Time : 326.79 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 7000 , Time : 323.47 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 8000 , Time : 325.03 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 9000 , Time : 324.66 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 10000 , Time : 324.82 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 11000 , Time : 323.99 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 12000 , Time : 325.76 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 13000 , Time : 323.93 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 14000 , Time : 326.50 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 15000 , Time : 323.00 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 16000 , Time : 321.57 , TrainLoss : 0.0000\n",
      "Epoch : 3 , Iteration : 17000 , Time : 325.20 , TrainLoss : 0.0000\n",
      ">> Epoch : 3 , TrainLoss : 0.0000 , EvalLoss : 0.0156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "clip = 2.0\n",
    "save_path = './ft_bert_0217.pt'\n",
    "best_eval_loss = float('inf')\n",
    "for epoch in range(1, 1+n_epochs):\n",
    "    model, optimizer, train_loss, eval_loss = train_epoch(model, device, epoch, \n",
    "                                                          train_dataloader, validation_dataloader, \n",
    "                                                          criterion, optimizer, clip=clip)\n",
    "\n",
    "    print(\">> Epoch : {} , TrainLoss : {:.4f} , EvalLoss : {:.4f}\\n\".format \\\n",
    "          (epoch, train_loss, eval_loss))\n",
    "\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_res(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            \n",
    "            if y_true is None:\n",
    "                y_true = b_labels\n",
    "            else:\n",
    "                y_true = torch.cat((y_true, b_labels), 0)\n",
    "            # outputs = model(seqs, seq_lens)\n",
    "\n",
    "            if y_pred is None:\n",
    "                y_pred = outputs\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred, outputs), 0)\n",
    "            # break\n",
    "\n",
    "    print(y_true.size(), y_pred.size())\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    return y_true.cpu().numpy(), y_pred.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_column_wise_auc(y_true, y_pred):\n",
    "    assert y_true.shape[1] == y_pred.shape[1],'Arrays must have the same dimension'\n",
    "    list_of_aucs = []\n",
    "    for column in range(y_true.shape[1]):\n",
    "        #print(sum(y_true[:,column]), sum(y_pred[:,column]))\n",
    "        if sum(y_true[:,column]) == 0:\n",
    "            continue\n",
    "        list_of_aucs.append(roc_auc_score(y_true[:,column],y_pred[:,column]))\n",
    "    # print(list_of_aucs)\n",
    "    return np.array(list_of_aucs).mean(), len((list_of_aucs))\n",
    "\n",
    "def fit_active_value(s1, active_value):\n",
    "    # return np.round(s1)\n",
    "    return (s1 > active_value).astype(int)\n",
    "\n",
    "def cut_max_num(s1, max_num):\n",
    "    max_ids = np.argsort(s1)[-max_num:]\n",
    "    s3 = np.zeros_like(s1)\n",
    "    # for i in max_ids:\n",
    "    #     s3[i] = 1\n",
    "    s3[max_ids] = 1\n",
    "    return s3\n",
    "\n",
    "def cal_avg_p_r(arr_true, arr_pred, max_num=6, active_value=0.05):\n",
    "    ps, rs = [], []\n",
    "    for i in range(arr_true.shape[0]):\n",
    "        t1, s1 = arr_true[i], arr_pred[i]\n",
    "        if sum(t1) <= 0:\n",
    "            continue\n",
    "        s2 = fit_active_value(s1, active_value)\n",
    "        if sum(s2) > max_num:\n",
    "            s2 = cut_max_num(s1, max_num)\n",
    "        # if sum(s2) > 4:\n",
    "        #     s2 = get_maxf_adj(s1, s2, adj_phrase_map, phrase_prob, cannot_be_only)\n",
    "        # s2 = fit_threshold(s1, thresholds)\n",
    "\n",
    "        p, r = precision_score(t1, s2), recall_score(t1, s2)\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "    return np.average(ps), np.average(rs), len(ps), ps, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48096, 133]) torch.Size([48096, 133])\n"
     ]
    }
   ],
   "source": [
    "val_true, val_pred = predict_res(model, validation_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93581, 133]) torch.Size([93581, 133])\n"
     ]
    }
   ],
   "source": [
    "# test_dataloader\n",
    "test_true, test_pred = predict_res(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9712245081369616, (0.9785304744549129, 129))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_auc = roc_auc_score(val_true, val_pred)\n",
    "test_auc = mean_column_wise_auc(test_true, test_pred) # 0.9775929439549857, 129\n",
    "# val_auc # 0.9718519698062188 -> 0.9726565791687066 # val\n",
    "# test_auc = roc_auc_score(test_true, test_pred)\n",
    "val_auc, test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5286673181414948, 0.9365043557065764, 48096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_p, avr_r, num, ps, rs = cal_avg_p_r(val_true, val_pred, max_num=6, active_value=0.02)\n",
    "avg_p, avr_r, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5222283369487396, 0.958959066612864, 93581)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavg_p, tavr_r, tnum, tps, trs = cal_avg_p_r(test_true, test_pred, max_num=6, active_value=0.02)\n",
    "tavg_p, tavr_r, tnum # (0.5230689118071864, 0.9624222365379759, 93581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-05512fd09d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tps' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(np.concatenate((ps, tps))), np.mean(np.concatenate((rs, trs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3006"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.05  (0.6115475516068873, 0.9200020758747057, 48096)\n",
    "# 0.03  (0.5474526120430204, 0.9372397730993041, 48096)\n",
    "# 0.025 (0.5391036126577576, 0.9401866241062848, 48096)\n",
    "# 0.02  (0.508811839430167, 0.9462131622997392, 48096)\n",
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_sampling(data, num=50):\n",
    "    _arr = np.asarray(data)\n",
    "    ids = np.random.choice(len(_arr), num, replace=False)\n",
    "    return _arr[ids].tolist()\n",
    "\n",
    "sample_ids = rand_sampling(range(93580), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "test_inputs_ = test_inputs.tolist()\n",
    "test_masks_ = test_masks.tolist()\n",
    "test_labels_ = test_labels.tolist()\n",
    "\n",
    "sample_lnis = []\n",
    "sample_inputs_ = []\n",
    "sample_masks_ = []\n",
    "sample_labels_ = []\n",
    "\n",
    "for i in sample_ids:\n",
    "    sample_labels_.append(test_labels_[i])\n",
    "    sample_inputs_.append(test_inputs_[i])\n",
    "    sample_masks_.append(test_masks_[i])\n",
    "    sample_lnis.append(test_lnis[i])\n",
    "    \n",
    "print(len(sample_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs = torch.tensor(sample_inputs_).long()\n",
    "sample_masks = torch.tensor(sample_masks_).long()\n",
    "sample_labels = torch.tensor(sample_labels_).float()\n",
    "\n",
    "sample_data = TensorDataset(sample_inputs, sample_masks, sample_labels)\n",
    "sample_sampler = SequentialSampler(sample_data)\n",
    "sample_dataloader = DataLoader(sample_data, sampler=sample_sampler, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 133]) torch.Size([1000, 133])\n",
      "213.7441337108612\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# cpu : 1000 - 214s\n",
    "start_time = time.time()\n",
    "sample_true, sample_pred = predict_res(model, sample_dataloader, device)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9808125936962412, 108)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_column_wise_auc(sample_true, sample_pred) # 0.9775929439549857, 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.54462, 0.9613180952380952, 5000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_p, avr_r, num, ps, rs = cal_avg_p_r(sample_true, sample_pred, max_num=6, active_value=0.02)\n",
    "avg_p, avr_r, num,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samle_res(sample_lnis, sample_true, sample_pred, i2phrase, max_num=6, active_value=0.05):\n",
    "    sample_show_res = []\n",
    "    for i, lni in enumerate(sample_lnis):\n",
    "        t1, s1 = sample_true[i], sample_pred[i]\n",
    "        if sum(t1) <= 0:\n",
    "            continue\n",
    "        s2 = fit_active_value(s1, active_value)\n",
    "        if sum(s2) > max_num:\n",
    "            s2 = cut_max_num(s1, max_num)\n",
    "\n",
    "        tids = np.where(t1 == 1)[0]\n",
    "        pids = np.where(s2 == 1)[0]\n",
    "\n",
    "        p, r = precision_score(t1, s2), recall_score(t1, s2)\n",
    "\n",
    "        true_p = ' # '.join(i2phrase[j] for j in tids)\n",
    "        pred_p = ' # '.join(i2phrase[j] for j in pids)\n",
    "        sample_show_res.append((lni, true_p, pred_p, p, r, len(tids), len(pids)))\n",
    "\n",
    "    return sample_show_res\n",
    "\n",
    "# sample_show_res = prepare_samle_res(sample_lnis, sample_true, sample_pred, i2phrase_rule_133, max_num=6, active_value=0.02)\n",
    "# print(len(sample_show_res))\n",
    "# print(sample_show_res[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is relate back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19893, 7834, 19996, 373, 48096)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cal_acc_relate_back(arr_true, arr_pred)\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "active_value = 0.02\n",
    "ps, rs = [], []\n",
    "for i in range(val_true.shape[0]):\n",
    "    t1, s1 = val_true[i], val_pred[i]\n",
    "    if sum(t1) <= 0:\n",
    "        continue\n",
    "    s2 = fit_active_value(s1, active_value)\n",
    "    tbool = is_relate_back(t1, i2phrase_rule_133, phrase_rule)\n",
    "    sbool = is_relate_back(s2, i2phrase_rule_133, phrase_rule)\n",
    "    if tbool and sbool:\n",
    "        tp += 1\n",
    "    elif tbool:\n",
    "        fn += 1\n",
    "    elif sbool:\n",
    "        fp += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "tp, fp, tn, fn, tp+fp+tn+fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relate_back(arr, i2phrase_rule_133, phrase_rule):\n",
    "    ids = np.where(arr == 1)[0]\n",
    "    bools = []\n",
    "    for i in ids:\n",
    "        p = i2phrase_rule_133[i]\n",
    "        pinfos = phrase_rule[p]\n",
    "        ibool = any([item.get('is_realte_back') == '1' for item in pinfos['letter'].values()])\n",
    "        bools.append(ibool)\n",
    "    return any(bools)\n",
    "\n",
    "def showp(arr, i2phrase_rule_133):\n",
    "    ids = np.where(arr == 1)[0]\n",
    "    for i in ids:\n",
    "        p = i2phrase_rule_133[i]\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FtBert(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=133, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./bert_models/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FtBert.from_pretrained(output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48096, 133]) torch.Size([48096, 133])\n"
     ]
    }
   ],
   "source": [
    "val_true, val_pred = predict_res(model, validation_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93581, 133]) torch.Size([93581, 133])\n"
     ]
    }
   ],
   "source": [
    "# test_dataloader\n",
    "test_true, test_pred = predict_res(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5018720268156032, (0.51028013505342, 129))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_auc = roc_auc_score(val_true, val_pred)\n",
    "test_auc = mean_column_wise_auc(test_true, test_pred)\n",
    "val_auc, test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016099744954535374, 0.07774541855442554, 48096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pr = cal_avg_p_r(val_true, val_pred, max_num=6, active_value=0.02)\n",
    "val_pr[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pr = cal_avg_p_r(test_true, test_pred, max_num=6, active_value=0.02)\n",
    "test_pr[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python36764bit13a3f6551b89447da3c8f3920b014f0d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-candidate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}