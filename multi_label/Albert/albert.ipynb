{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0) # won't use cuda:0 to initialize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (AlbertConfig, AlbertTokenizer, AlbertPreTrainedModel, AdamW, AlbertModel,\n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 10975,    15,    51,  1952,    25, 10901,     3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e9698b73e44eb2b76f2bbf3def594a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=535, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6922e3b0713b4f268ba4fd77b7d4ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=236197176, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AlbertModel.from_pretrained('albert-xlarge-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids)\n",
    "pooled_output = outputs[1]  # The last hidden-state is the first element of the output tuple\n",
    "pooled_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlBertForClassification(AlbertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.albert = AlbertModel(config)\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        # self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # self.init_weights() # change initial weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1] # (bs, hidden_size)\n",
    "        # pooled_output = F.relu(self.pre_classifier(pooled_output))  # (bs, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlBertForClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=133, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = AlbertConfig(hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    num_labels=133)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlBertForClassification(configuration)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight torch.Size([768, 768]) True tensor([ 0.0042,  0.0179, -0.0147,  0.0340,  0.0254, -0.0347,  0.0232, -0.0201,\n",
      "        -0.0471, -0.0234], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "for p in params[9:]:\n",
    "    print(p[0], p[1].size(), p[1].requires_grad, p[1][0, :10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30000, 128]) True\n",
      "embeddings.position_embeddings.weight torch.Size([512, 128]) True\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 128]) True\n",
      "embeddings.LayerNorm.weight torch.Size([128]) True\n",
      "embeddings.LayerNorm.bias torch.Size([128]) True\n",
      "encoder.embedding_hidden_mapping_in.weight torch.Size([768, 128]) True\n",
      "encoder.embedding_hidden_mapping_in.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight torch.Size([768, 768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight torch.Size([768, 768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight torch.Size([768, 768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight torch.Size([768, 768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias torch.Size([768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn.weight torch.Size([3072, 768]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn.bias torch.Size([3072]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight torch.Size([768, 3072]) True\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias torch.Size([768]) True\n",
      "pooler.weight torch.Size([768, 768]) True\n",
      "pooler.bias torch.Size([768]) True\n"
     ]
    }
   ],
   "source": [
    "for p in params[:]:\n",
    "    print(p[0], p[1].size(), p[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, epoch, train_dataloader, validation_dataloader, \n",
    "                criterion, optimizer, scheduler, clip=5.):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    t0 = time.time()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader, 1):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            # print loss info every 20 Iterations\n",
    "            log_str = \"Epoch : {} , Iteration : {} , Time : {:.2f} , TrainLoss : {:.9f}\".format \\\n",
    "                        (epoch, step, time.time()-t0, train_loss/step)\n",
    "            print(log_str)\n",
    "            t0 = time.time()\n",
    "            break\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(validation_dataloader, 1):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            loss = criterion(outputs, b_labels)\n",
    "            eval_loss += loss.item()\n",
    "            break\n",
    "        eval_loss /= len(validation_dataloader)\n",
    "\n",
    "    return model, optimizer, train_loss, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * n_epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "clip = 2.0\n",
    "for epoch in range(1, 1+n_epochs):\n",
    "    model, optimizer, train_loss, eval_loss = train_epoch(model, device, epoch, \n",
    "                                                          train_dataloader, validation_dataloader, \n",
    "                                                          criterion, optimizer, scheduler, clip=clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# albert-xlarge-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cuda out of memory\n",
    "# class AlBertForClassification(AlbertPreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         # self.albert = AlbertModel(config)\n",
    "#         self.albert = AlbertModel.from_pretrained('albert-xlarge-v2')\n",
    "#         # self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "#         self.dropout = nn.Dropout(config.classifier_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "#         # self.init_weights() # change initial weights\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None):\n",
    "#         outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs[1] # (bs, hidden_size)\n",
    "#         # pooled_output = F.relu(self.pre_classifier(pooled_output))  # (bs, hidden_size)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlBertForClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (key): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (value): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (LayerNorm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (ffn_output): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=2048, out_features=133, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration = AlbertConfig(hidden_size=2048,\n",
    "#     num_attention_heads=16,\n",
    "#     num_hidden_layers=24, \n",
    "#     intermediate_size=8192,\n",
    "#     num_labels=133)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = AlBertForClassification(configuration)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362560"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = './model_data.pkl'\n",
    "with open(train_path, 'rb') as f:\n",
    "    traindata = pickle.load(f)\n",
    "len(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tempdata, phrase_rule_133, max_sent_num=6):\n",
    "    half_ = max_sent_num // 2\n",
    "    lnis = []\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    num_classes = len(phrase_rule_133)\n",
    "    for lni, items in tempdata.items():\n",
    "        label = [0] * num_classes\n",
    "        phrase = [phrase_rule_133[p] for p in items['phrases'] if p in phrase_rule_133]\n",
    "        if not phrase:\n",
    "            continue\n",
    "\n",
    "        if len(items['pred_sents']) > max_sent_num:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'][:half_] + items['pred_sents'][-half_:])\n",
    "        else:\n",
    "            sent = ' '.join(s for _, s in items['pred_sents'])\n",
    "\n",
    "        if not sent:\n",
    "            continue\n",
    "\n",
    "        inputs.append(sent)\n",
    "\n",
    "        for i in set(phrase):\n",
    "            label[i] = 1\n",
    "        assert sum(label) > 0\n",
    "        labels.append(label[:])\n",
    "        lnis.append(lni)\n",
    "\n",
    "    print(len(labels), len(inputs), len(lnis))\n",
    "    return lnis, inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320635 320635 320635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"denise wilson has filed a motion for an extension of time to file objections to the magistrate judge's proposed findings and recommended disposition . that motion is granted . document .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, inputs, labels = prepare_data(traindata, phrase_rule_133)\n",
    "inputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112089\n",
      "93581 93581 93581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"extension of time granted to april , to file the reply to the informal response to the petition for writ of habeas corpus . extension is granted based upon deputy state public defender debra s . sabah press's representation that she anticipates filing that brief by .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = './model_data_ftc.pkl'\n",
    "with open(test_path, 'rb') as f:\n",
    "    testdata = pickle.load(f)\n",
    "print(len(testdata))\n",
    "\n",
    "test_lnis, test_inputs, test_labels = prepare_data(testdata, phrase_rule_133)\n",
    "test_inputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93581"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent2tokenids(inputs, tokenizer):\n",
    "    input_ids = [] # List[List[int]]\n",
    "\n",
    "    for i, sent in enumerate(inputs):\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_sent)\n",
    "    # print(len(input_ids))\n",
    "    return input_ids\n",
    "\n",
    "# test_input_ids = sent2tokenids(test_inputs, tokenizer)\n",
    "len(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17871, 2571, 63, 5263, 21, 2422, 26, 40, 3896, 16, 85, 20, 3893, 20045, 20, 14, 12393, 1878, 22, 18, 2097, 10172, 17, 5773, 22157, 13, 9, 30, 2422, 25, 2743, 13, 9, 4492, 13, 9, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=320635, minmax=(3, 13206), mean=141.37591965942582, variance=15473.276440663463, skewness=4.906607916199845, kurtosis=384.89478880556277)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_input_ids = sent2tokenids(inputs, tokenizer)\n",
    "\n",
    "print(train_input_ids[10])\n",
    "lengths = [len(x) for x in train_input_ids]\n",
    "stats.describe(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_mask(new_inp_ids, pad_id):\n",
    "    assert pad_id == 0, 'bug of pad id'\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # For each sentence...\n",
    "    for sent in new_inp_ids:\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > pad_id) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "    print('Length of Attention mask : {}'.format(len(attention_masks)))\n",
    "    return attention_masks\n",
    "\n",
    "\n",
    "def truncate_pad_id(input_ids, tokenizer, max_len=256):\n",
    "    half_len = max_len // 2\n",
    "\n",
    "    # truncation and padding\n",
    "    pad_id = tokenizer.pad_token_id # 0\n",
    "\n",
    "    new_inp_ids = []\n",
    "    for x in input_ids:\n",
    "        ll = len(x)\n",
    "        if ll > max_len:\n",
    "            new_inp_ids.append(x[:half_len] + x[-half_len:])\n",
    "        elif ll < max_len:\n",
    "            new_inp_ids.append(x + [pad_id] * (max_len-ll))\n",
    "        else:\n",
    "            new_inp_ids.append(x)\n",
    "        assert len(new_inp_ids[-1]) == max_len\n",
    "\n",
    "    print('Length of inputs : {}'.format(len(new_inp_ids)))\n",
    "    attention_masks = prepare_attention_mask(new_inp_ids, pad_id)\n",
    "    return new_inp_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs : 320635\n",
      "Length of Attention mask : 320635\n"
     ]
    }
   ],
   "source": [
    "new_train_inp_ids, train_attention_masks = truncate_pad_id(train_input_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs : 93581\n",
      "Length of Attention mask : 93581\n"
     ]
    }
   ],
   "source": [
    "new_test_inp_ids, test_attention_masks = truncate_pad_id(test_input_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272539, 272539, 272539, 48096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(new_train_inp_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.15)\n",
    "# Do the same for the masks.\n",
    "train_masks, val_masks, _, _ = train_test_split(train_attention_masks, labels,\n",
    "                                             random_state=2020, test_size=0.15)\n",
    "\n",
    "len(train_inputs), len(train_masks), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs).long()\n",
    "val_inputs = torch.tensor(val_inputs).long()\n",
    "\n",
    "train_labels = torch.tensor(train_labels).float()\n",
    "val_labels = torch.tensor(val_labels).float()\n",
    "\n",
    "train_masks = torch.tensor(train_masks).long()\n",
    "val_masks = torch.tensor(val_masks).long()\n",
    "\n",
    "test_inputs = torch.tensor(new_test_inp_ids).long()\n",
    "test_masks = torch.tensor(test_attention_masks).long()\n",
    "test_labels = torch.tensor(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _process_data = {'train_inputs': train_inputs, 'val_inputs': val_inputs, 'test_inputs': test_inputs, \n",
    "#                  'train_labels': train_labels, 'val_labels': val_labels, 'test_labels': test_labels, \n",
    "#                  'train_masks': train_masks, 'val_masks': val_masks, 'test_masks': test_masks, \n",
    "#                  'test_lnis': test_lnis}\n",
    "# with open('/data/charley/half_data/e3_albert_process_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(_process_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "with open('./process_data.pkl', 'rb') as f:\n",
    "    _process_data = pickle.load(f)\n",
    "    \n",
    "train_inputs = _process_data['train_inputs']\n",
    "train_labels = _process_data['train_labels']\n",
    "train_masks = _process_data['train_masks']\n",
    "\n",
    "val_inputs = _process_data['val_inputs']\n",
    "val_labels = _process_data['val_labels']\n",
    "val_masks = _process_data['val_masks']\n",
    "\n",
    "# test_inputs, test_masks = _process_data['test_inputs'], _process_data['test_masks']\n",
    "# test_labels, test_lnis = _process_data['test_labels'], _process_data['test_lnis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "# len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charley",
   "language": "python",
   "name": "charley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}