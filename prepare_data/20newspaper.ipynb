{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "newsgroups_all = fetch_20newsgroups(subset='all').data\n",
    "labels = fetch_20newsgroups(subset='all').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  3 17  3  4 12  4 10 10 19 19 11 19 13  0 17 12 12 11  8  7  5  1  8\n",
      " 10 14 16  1  6  0  7 16  5  9 13  4  4 18  8  8 19  1 12  7 10  5  2  6\n",
      " 11  2]\n"
     ]
    }
   ],
   "source": [
    "print(labels[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from mamatha devineni ratnam mr47 andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non pittsburghers relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the pulp out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule', 'from mblawson midway ecn uoknor edu matthew b lawson subject which high performance vlb video card summary seek recommendations for vlb video card nntp posting host midway ecn uoknor edu organization engineering computer network university of oklahoma norman ok usa keywords orchid stealth vlb lines 21 my brother is in the market for a high performance video card that supports vesa local bus with 1 2mb ram does anyone have suggestions ideas on diamond stealth pro local bus orchid farenheit 1280 ati graphics ultra pro any other high performance vlb card please post or email thank you matt matthew b lawson mblawson essex ecn uoknor edu now i nebuchadnezzar praise and exalt and glorify the king of heaven because everything he does is right and all his ways are just nebuchadnezzar king of babylon 562 b c']\n"
     ]
    }
   ],
   "source": [
    "sents = [normalize_string(s) for s in newsgroups_all]\n",
    "print(sents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, sents, stoplist, min_count=30):\n",
    "        self.sents = sents\n",
    "        self.stoplist = stoplist\n",
    "        self.min_count = min_count\n",
    "        self.word2idx = {\"<PAD>\": 0}\n",
    "        self.idx2word = {0: \"<PAD>\"}\n",
    "        self.n_words = self.process_sents()\n",
    "\n",
    "    def process_sents(self):\n",
    "        words = []\n",
    "        for sent in self.sents:\n",
    "            words += sent.split(' ')\n",
    "\n",
    "        cc = 1\n",
    "        counter = Counter(words)\n",
    "        for word, num in counter.items():\n",
    "            if num > self.min_count and word not in self.stoplist:\n",
    "                self.word2idx[word] = cc\n",
    "                self.idx2word[cc] = word\n",
    "                cc += 1\n",
    "        return cc\n",
    "    \n",
    "# input_lang = Lang(sents, stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words == len(input_lang.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12693"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = [[input_lang.word2idx[word] for word in sent.split(' ') if word in input_lang.word2idx] for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 21, 5, 6, 22, 23, 24, 25, 26, 27, 5, 28, 29, 30, 31, 31, 32, 33, 34, 35, 36, 37, 31, 38, 5, 39, 40, 29, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 43, 51, 53, 58, 59, 60, 61, 5, 33, 62, 63, 64, 65, 66, 67, 68, 69, 47, 48, 70, 5, 71], [72, 73, 74, 3, 75, 76, 4, 77, 78, 79, 80, 81, 82, 83, 84, 79, 80, 81, 17, 18, 19, 72, 73, 74, 3, 8, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 79, 15, 96, 97, 98, 77, 78, 80, 81, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 95, 110, 101, 102, 94, 111, 112, 113, 110, 77, 78, 79, 81, 114, 9, 115, 116, 117, 75, 76, 73, 74, 3, 38, 118, 119, 120, 121, 122, 118, 123, 76, 124]]\n"
     ]
    }
   ],
   "source": [
    "print(input_seqs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167.4241748912236, 471.4871182567455)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(seq) for seq in input_seqs]\n",
    "np.mean(lengths), np.std(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len = 1000\n",
    "input_seqs = [seq[:max_sent_len] for seq in input_seqs]\n",
    "lengths = [len(seq) for seq in input_seqs]\n",
    "np.max(lengths), np.min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11307, 7539)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(input_seqs) == len(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_seqs, labels, test_size=0.4, shuffle=False)\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11829 words with pre-trained vector in vocab_size = 12693.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/data/charley/crawl-300d-2M.pkl', 'rb') as pf:\n",
    "    fb_w2v = pickle.load(pf)\n",
    "    \n",
    "embed_dim = 300\n",
    "embed_matrix = np.zeros((input_lang.n_words, embed_dim))\n",
    "flag = 0\n",
    "for word, idx in input_lang.word2idx.items():\n",
    "    try:\n",
    "        word_vec = fb_w2v[word]\n",
    "    except:\n",
    "        word_vec = None\n",
    "    if word_vec is not None:\n",
    "        embed_matrix[idx] = word_vec\n",
    "        flag += 1\n",
    "print(\"There are {} words with pre-trained vector in vocab_size = {}.\".format(flag, len(input_lang.word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {'train': {'x': x_train, 'y': y_train}, \n",
    "              'test': {'x': x_test, 'y': y_test}, \n",
    "              'lang': input_lang}\n",
    "\n",
    "with open('newspaper_data.pkl', 'wb') as wf:\n",
    "    pickle.dump(data_split, wf)\n",
    "    \n",
    "np.save('EmbeddingMatrix', embed_matrix) # saved as EmbeddingMatrix.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('newspaper_data.pkl', 'rb') as rf:\n",
    "    data_split = pickle.load(rf)\n",
    "    \n",
    "x_train, y_train = data_split['train']['x'], data_split['train']['y']\n",
    "x_test, y_test = data_split['test']['x'], data_split['test']['y']\n",
    "embed_matrix = np.load('EmbeddingMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape[0] == len(data_split['lang'].word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def collate_fn(insts, PAD_token=0):\n",
    "    # if seq_pad in class then all seqs with same length\n",
    "    maxlen = max([len(x) for x in insts])\n",
    "    #maxlen = 24\n",
    "    seq = np.array([x + [PAD_token] * (maxlen - len(x)) for x in insts])\n",
    "    seq_lens = np.array([len(x) for x in insts])\n",
    "    return torch.LongTensor(seq), torch.LongTensor(seq_lens)\n",
    "\n",
    "def paired_collate_fn(insts):\n",
    "    #src_insts, tgt_insts = list(zip(*insts))\n",
    "    seq_pairs = sorted(insts, key=lambda p: len(p[0]), reverse=True)\n",
    "    src_insts, tgt_insts = zip(*seq_pairs)\n",
    "    src_insts = collate_fn(src_insts)\n",
    "    # tgt_insts = collate_fn(tgt_insts)\n",
    "    return (*src_insts, tgt_insts)\n",
    "\n",
    "class NewsPaperDatasets(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        # self.device = device\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "tensor([10,  5, 11, 18, 14, 17, 10,  9, 17, 14,  0, 10, 17, 13, 18,  3, 19,  2,\n",
      "        15,  4, 13,  8,  5,  7,  3, 19, 17,  5, 17,  3, 18, 16, 17,  5, 16, 14,\n",
      "        11, 17, 13,  8,  9, 15,  0, 12,  7, 12,  0, 18, 14,  2, 10,  3,  8,  4,\n",
      "         7,  5,  0, 14, 10,  2, 14, 11,  2,  5, 15,  1,  9,  0,  7, 18,  1,  0,\n",
      "        18,  2, 10, 14, 12, 15,  7, 16,  2,  3, 12, 17,  7, 14,  9,  7, 14, 15,\n",
      "        10, 18,  8,  9,  0, 11, 10,  4,  0,  4, 15, 13,  8,  2, 17, 15, 19, 18,\n",
      "        16, 17, 11, 16,  4, 13, 18, 15, 15, 15,  9,  3,  4, 11,  6, 14, 11,  1,\n",
      "        10, 11,  4,  2, 19, 12, 19, 14, 10, 18, 17,  6,  5, 12,  7, 17,  8,  5,\n",
      "         7, 19, 16,  3,  4,  1,  6, 19,  2,  7,  5,  9,  1,  3,  4,  2, 11, 12,\n",
      "        12, 14,  1,  6,  6,  2,  0, 14,  4,  3, 18,  1,  3,  7,  7,  5, 19, 12,\n",
      "         8,  6, 13, 10,  7,  7,  0,  3, 11, 19, 12,  6,  1,  1,  7,  7,  4,  6,\n",
      "         8,  5,  5,  5,  3,  5, 15, 19, 11,  3,  2,  7,  4,  9, 13, 14, 15, 13,\n",
      "         1, 12,  9,  4, 15, 18, 18,  4,  9,  7,  9, 14,  1,  7,  0,  4,  1, 10,\n",
      "         2, 11,  7, 19, 10,  6, 17,  0, 17,  7, 15,  4,  7,  3,  4, 13, 13, 13,\n",
      "        13, 18,  7,  1])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    NewsPaperDatasets(x_train, y_train),\n",
    "                    num_workers = 2,\n",
    "                    batch_size = 256,\n",
    "                    collate_fn = paired_collate_fn,\n",
    "                    shuffle = True,\n",
    "                    drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    NewsPaperDatasets(x_test, y_test),\n",
    "                    num_workers = 2,\n",
    "                    batch_size = 256,\n",
    "                    collate_fn = paired_collate_fn,\n",
    "                    shuffle = True,\n",
    "                    drop_last = True)\n",
    "\n",
    "for x in test_loader:\n",
    "    src_var, src_len, tgt = x\n",
    "\n",
    "    tgt = torch.tensor(tgt)\n",
    "#     print(src_len)\n",
    "    print(tgt.size())\n",
    "    print(tgt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, device, weights, embed_dim=300, batch_size=256, \n",
    "                 hidden_size=256, n_layers=1, dropout=0.5, output_size=20):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=False)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, n_layers, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_var, input_len, hidden=None):\n",
    "        embeded = self.embed(input_var)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeded, input_len)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, out_len = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "        outputs = F.relu(self.dropout( torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) ))\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers * 2, self.batch_size, self.hidden_size, device=self.device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, train_loader, test_loader, criterion, optimizer, clip=5., batch_sz=256):\n",
    "    # print(\"There are {} batches in one epoch.\".format( len(train_loader) ))\n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    train_loss = 0\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        src, src_lens, tgt = batch\n",
    "        src = src.permute(1, 0).to(model.device)\n",
    "        tgt = torch.tensor(tgt).to(model.device) # tgt without modified - cuda out of memory\n",
    "        \n",
    "        optimizer.zero_grad() # here same as optimizer.zero_grad()\n",
    "        hidden = hidden.detach()\n",
    "        outputs, hidden = model(src, src_lens, hidden)\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            # print loss info every 20 Iterations\n",
    "            log_str = \"Epoch : {} , Iteration : {} , Time : {:.2f} , TrainLoss : {:.4f}\".format \\\n",
    "                        (epoch, i, time.time()-t0, train_loss/i)\n",
    "            print(log_str)\n",
    "            t0 = time.time()\n",
    "            \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    # print(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    \n",
    "    corr = total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader, 1):\n",
    "            src, src_lens, tgt = batch\n",
    "            total += len(tgt)\n",
    "            src = src.permute(1, 0).to(model.device)\n",
    "            tgt = torch.tensor(tgt).to(model.device)\n",
    "            outputs, _ = model(src, src_lens, hidden)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            corr += (pred.data.cpu().numpy() == tgt.data.cpu().numpy()).sum()\n",
    "\n",
    "        eval_loss = eval_loss / len(test_loader)\n",
    "        \n",
    "        accuracy = corr / total\n",
    "    return model, train_loss, eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleGRU(\n",
       "  (embed): Embedding(12693, 300)\n",
       "  (gru): GRU(300, 256, num_layers=2, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sgru = SimpleGRU(device, embed_matrix, n_layers=2)\n",
    "sgru.to(sgru.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Iteration : 10 , Time : 3.26 , TrainLoss : 2.9697\n",
      "Epoch : 1 , Iteration : 20 , Time : 3.28 , TrainLoss : 2.9308\n",
      "Epoch : 1 , Iteration : 30 , Time : 3.25 , TrainLoss : 2.8399\n",
      "Epoch : 1 , Iteration : 40 , Time : 3.30 , TrainLoss : 2.7125\n",
      ">> Epoch : 1 , TrainLoss : 2.6575 , EvalLoss : 1.9452 , EvalAccuracy : 0.3397\n",
      "\n",
      "\n",
      "Epoch : 2 , Iteration : 10 , Time : 3.49 , TrainLoss : 1.8446\n",
      "Epoch : 2 , Iteration : 20 , Time : 3.27 , TrainLoss : 1.7195\n",
      "Epoch : 2 , Iteration : 30 , Time : 3.27 , TrainLoss : 1.5991\n",
      "Epoch : 2 , Iteration : 40 , Time : 3.35 , TrainLoss : 1.4950\n",
      ">> Epoch : 2 , TrainLoss : 1.4570 , EvalLoss : 1.0404 , EvalAccuracy : 0.6117\n",
      "\n",
      "\n",
      "Epoch : 3 , Iteration : 10 , Time : 3.45 , TrainLoss : 0.8890\n",
      "Epoch : 3 , Iteration : 20 , Time : 3.34 , TrainLoss : 0.8428\n",
      "Epoch : 3 , Iteration : 30 , Time : 3.29 , TrainLoss : 0.8325\n",
      "Epoch : 3 , Iteration : 40 , Time : 3.32 , TrainLoss : 0.8045\n",
      ">> Epoch : 3 , TrainLoss : 0.7936 , EvalLoss : 0.7362 , EvalAccuracy : 0.7303\n",
      "\n",
      "\n",
      "Epoch : 4 , Iteration : 10 , Time : 3.48 , TrainLoss : 0.5209\n",
      "Epoch : 4 , Iteration : 20 , Time : 3.30 , TrainLoss : 0.4930\n",
      "Epoch : 4 , Iteration : 30 , Time : 3.31 , TrainLoss : 0.4845\n",
      "Epoch : 4 , Iteration : 40 , Time : 3.29 , TrainLoss : 0.4759\n",
      ">> Epoch : 4 , TrainLoss : 0.4700 , EvalLoss : 0.5966 , EvalAccuracy : 0.7833\n",
      "\n",
      "\n",
      "Epoch : 5 , Iteration : 10 , Time : 3.47 , TrainLoss : 0.3098\n",
      "Epoch : 5 , Iteration : 20 , Time : 3.30 , TrainLoss : 0.2956\n",
      "Epoch : 5 , Iteration : 30 , Time : 3.18 , TrainLoss : 0.2926\n",
      "Epoch : 5 , Iteration : 40 , Time : 3.33 , TrainLoss : 0.2906\n",
      ">> Epoch : 5 , TrainLoss : 0.2894 , EvalLoss : 0.5591 , EvalAccuracy : 0.7951\n",
      "\n",
      "\n",
      "Epoch : 6 , Iteration : 10 , Time : 3.47 , TrainLoss : 0.2301\n",
      "Epoch : 6 , Iteration : 20 , Time : 3.35 , TrainLoss : 0.2125\n",
      "Epoch : 6 , Iteration : 30 , Time : 3.33 , TrainLoss : 0.2026\n",
      "Epoch : 6 , Iteration : 40 , Time : 3.30 , TrainLoss : 0.1993\n",
      ">> Epoch : 6 , TrainLoss : 0.1977 , EvalLoss : 0.4940 , EvalAccuracy : 0.8211\n",
      "\n",
      "\n",
      "Epoch : 7 , Iteration : 10 , Time : 3.48 , TrainLoss : 0.1312\n",
      "Epoch : 7 , Iteration : 20 , Time : 3.33 , TrainLoss : 0.1335\n",
      "Epoch : 7 , Iteration : 30 , Time : 3.31 , TrainLoss : 0.1349\n",
      "Epoch : 7 , Iteration : 40 , Time : 3.25 , TrainLoss : 0.1333\n",
      ">> Epoch : 7 , TrainLoss : 0.1343 , EvalLoss : 0.5124 , EvalAccuracy : 0.8221\n",
      "\n",
      "\n",
      "Epoch : 8 , Iteration : 10 , Time : 3.51 , TrainLoss : 0.0993\n",
      "Epoch : 8 , Iteration : 20 , Time : 3.28 , TrainLoss : 0.1074\n",
      "Epoch : 8 , Iteration : 30 , Time : 3.31 , TrainLoss : 0.1049\n",
      "Epoch : 8 , Iteration : 40 , Time : 3.34 , TrainLoss : 0.1018\n",
      ">> Epoch : 8 , TrainLoss : 0.1012 , EvalLoss : 0.5166 , EvalAccuracy : 0.8221\n",
      "\n",
      "\n",
      "Epoch : 9 , Iteration : 10 , Time : 3.46 , TrainLoss : 0.0766\n",
      "Epoch : 9 , Iteration : 20 , Time : 3.27 , TrainLoss : 0.0754\n",
      "Epoch : 9 , Iteration : 30 , Time : 3.29 , TrainLoss : 0.0723\n",
      "Epoch : 9 , Iteration : 40 , Time : 3.29 , TrainLoss : 0.0711\n",
      ">> Epoch : 9 , TrainLoss : 0.0721 , EvalLoss : 0.5173 , EvalAccuracy : 0.8264\n",
      "\n",
      "\n",
      "Epoch : 10 , Iteration : 10 , Time : 3.47 , TrainLoss : 0.0639\n",
      "Epoch : 10 , Iteration : 20 , Time : 3.28 , TrainLoss : 0.0663\n",
      "Epoch : 10 , Iteration : 30 , Time : 3.29 , TrainLoss : 0.0692\n",
      "Epoch : 10 , Iteration : 40 , Time : 3.18 , TrainLoss : 0.0676\n",
      ">> Epoch : 10 , TrainLoss : 0.0673 , EvalLoss : 0.6118 , EvalAccuracy : 0.8087\n",
      "\n",
      "\n",
      "Epoch : 11 , Iteration : 10 , Time : 3.41 , TrainLoss : 0.0503\n",
      "Epoch : 11 , Iteration : 20 , Time : 3.27 , TrainLoss : 0.0474\n",
      "Epoch : 11 , Iteration : 30 , Time : 3.17 , TrainLoss : 0.0474\n",
      "Epoch : 11 , Iteration : 40 , Time : 3.28 , TrainLoss : 0.0436\n",
      ">> Epoch : 11 , TrainLoss : 0.0447 , EvalLoss : 0.5535 , EvalAccuracy : 0.8257\n",
      "\n",
      "\n",
      "Epoch : 12 , Iteration : 10 , Time : 3.51 , TrainLoss : 0.0333\n",
      "Epoch : 12 , Iteration : 20 , Time : 3.29 , TrainLoss : 0.0341\n",
      "Epoch : 12 , Iteration : 30 , Time : 3.27 , TrainLoss : 0.0354\n",
      "Epoch : 12 , Iteration : 40 , Time : 3.30 , TrainLoss : 0.0346\n",
      ">> Epoch : 12 , TrainLoss : 0.0339 , EvalLoss : 0.5499 , EvalAccuracy : 0.8266\n",
      "\n",
      "\n",
      "Epoch : 13 , Iteration : 10 , Time : 3.44 , TrainLoss : 0.0279\n",
      "Epoch : 13 , Iteration : 20 , Time : 3.32 , TrainLoss : 0.0257\n",
      "Epoch : 13 , Iteration : 30 , Time : 3.30 , TrainLoss : 0.0265\n",
      "Epoch : 13 , Iteration : 40 , Time : 3.29 , TrainLoss : 0.0250\n",
      ">> Epoch : 13 , TrainLoss : 0.0256 , EvalLoss : 0.5632 , EvalAccuracy : 0.8297\n",
      "\n",
      "\n",
      "Epoch : 14 , Iteration : 10 , Time : 3.40 , TrainLoss : 0.0214\n",
      "Epoch : 14 , Iteration : 20 , Time : 3.26 , TrainLoss : 0.0202\n",
      "Epoch : 14 , Iteration : 30 , Time : 3.28 , TrainLoss : 0.0192\n",
      "Epoch : 14 , Iteration : 40 , Time : 3.30 , TrainLoss : 0.0196\n",
      ">> Epoch : 14 , TrainLoss : 0.0194 , EvalLoss : 0.5805 , EvalAccuracy : 0.8276\n",
      "\n",
      "\n",
      "Epoch : 15 , Iteration : 10 , Time : 3.43 , TrainLoss : 0.0107\n",
      "Epoch : 15 , Iteration : 20 , Time : 3.26 , TrainLoss : 0.0160\n",
      "Epoch : 15 , Iteration : 30 , Time : 3.17 , TrainLoss : 0.0176\n",
      "Epoch : 15 , Iteration : 40 , Time : 3.28 , TrainLoss : 0.0167\n",
      ">> Epoch : 15 , TrainLoss : 0.0181 , EvalLoss : 0.6109 , EvalAccuracy : 0.8239\n",
      "\n",
      "\n",
      "Epoch : 16 , Iteration : 10 , Time : 3.45 , TrainLoss : 0.0190\n",
      "Epoch : 16 , Iteration : 20 , Time : 3.27 , TrainLoss : 0.0181\n",
      "Epoch : 16 , Iteration : 30 , Time : 3.27 , TrainLoss : 0.0155\n",
      "Epoch : 16 , Iteration : 40 , Time : 3.22 , TrainLoss : 0.0139\n",
      ">> Epoch : 16 , TrainLoss : 0.0140 , EvalLoss : 0.5891 , EvalAccuracy : 0.8304\n",
      "\n",
      "\n",
      "Epoch : 17 , Iteration : 10 , Time : 3.50 , TrainLoss : 0.0195\n",
      "Epoch : 17 , Iteration : 20 , Time : 3.26 , TrainLoss : 0.0185\n",
      "Epoch : 17 , Iteration : 30 , Time : 3.28 , TrainLoss : 0.0172\n",
      "Epoch : 17 , Iteration : 40 , Time : 3.30 , TrainLoss : 0.0164\n",
      ">> Epoch : 17 , TrainLoss : 0.0165 , EvalLoss : 0.6098 , EvalAccuracy : 0.8244\n",
      "\n",
      "\n",
      "Epoch : 18 , Iteration : 10 , Time : 3.38 , TrainLoss : 0.0165\n",
      "Epoch : 18 , Iteration : 20 , Time : 3.30 , TrainLoss : 0.0153\n",
      "Epoch : 18 , Iteration : 30 , Time : 3.29 , TrainLoss : 0.0143\n",
      "Epoch : 18 , Iteration : 40 , Time : 3.25 , TrainLoss : 0.0151\n",
      ">> Epoch : 18 , TrainLoss : 0.0148 , EvalLoss : 0.6562 , EvalAccuracy : 0.8222\n",
      "\n",
      "\n",
      "Epoch : 19 , Iteration : 10 , Time : 3.40 , TrainLoss : 0.0118\n",
      "Epoch : 19 , Iteration : 20 , Time : 3.20 , TrainLoss : 0.0122\n",
      "Epoch : 19 , Iteration : 30 , Time : 3.24 , TrainLoss : 0.0132\n",
      "Epoch : 19 , Iteration : 40 , Time : 3.30 , TrainLoss : 0.0137\n",
      ">> Epoch : 19 , TrainLoss : 0.0136 , EvalLoss : 0.6270 , EvalAccuracy : 0.8293\n",
      "\n",
      "\n",
      "Epoch : 20 , Iteration : 10 , Time : 3.54 , TrainLoss : 0.0212\n",
      "Epoch : 20 , Iteration : 20 , Time : 3.28 , TrainLoss : 0.0213\n",
      "Epoch : 20 , Iteration : 30 , Time : 3.29 , TrainLoss : 0.0205\n",
      "Epoch : 20 , Iteration : 40 , Time : 3.23 , TrainLoss : 0.0183\n",
      ">> Epoch : 20 , TrainLoss : 0.0187 , EvalLoss : 0.6457 , EvalAccuracy : 0.8203\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(sgru.parameters())\n",
    "\n",
    "n_epochs = 20\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, 1+n_epochs):\n",
    "    sgru, train_loss, eval_loss, eval_acc = train_epoch(sgru, epoch, train_loader, test_loader, criterion, optimizer)\n",
    "    \n",
    "    print(\">> Epoch : {} , TrainLoss : {:.4f} , EvalLoss : {:.4f} , EvalAccuracy : {:.4f}\\n\\n\".format \\\n",
    "          (epoch, train_loss, eval_loss, eval_acc))\n",
    "    \n",
    "    if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            torch.save(sgru.state_dict(), 'sgru_newspaper_0114.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('data.pkl', 'rb') as rf:\n",
    "    data_split = pickle.load(rf)\n",
    "    \n",
    "x_train, y_train = data_split['train']['x'], data_split['train']['y']\n",
    "x_test, y_test = data_split['test']['x'], data_split['test']['y']\n",
    "input_lang = data_split['lang']\n",
    "\n",
    "embed_matrix = np.load('EmbeddingMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257, 216, 215, 207, 122, 104, 81, 43]\n",
      "(17, 10, 11, 15, 4, 2, 10, 7)\n"
     ]
    }
   ],
   "source": [
    "def get_batches(x, y, batch_size=8):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[ : (n_batches * batch_size)]\n",
    "    y = y[ : (n_batches * batch_size)]\n",
    "    for i in range(0, n_batches * batch_size, batch_size):\n",
    "        bx, by = x[i : (i+batch_size)], y[i : (i+batch_size)]\n",
    "        bxy = sorted(zip(bx, by), key=lambda p: len(p[0]), reverse=True)\n",
    "        bx, by = zip(*bxy)\n",
    "        bx_len = [len(p) for p in bx]\n",
    "        yield bx, bx_len, by\n",
    "        \n",
    "for batch in get_batches(x_test, y_test, batch_size=8):\n",
    "    bx, bx_len, by = batch\n",
    "    print(bx_len)\n",
    "    print(by)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "options_file = \"./small_emlo/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"./small_emlo/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "class ELMoGRU(nn.Module):\n",
    "    def __init__(self, device, options_file, weight_file, embed_dim=256, batch_size=64, \n",
    "                 hidden_size=256, n_layers=1, dropout=0.5, output_size=20):\n",
    "        super(ELMoGRU, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        # self.elmo.weight.requires_grad = False\n",
    "        # self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, n_layers, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seqs, input_len, hidden):\n",
    "        \n",
    "        character_ids = batch_to_ids(input_seqs).to(self.device)\n",
    "        embeded = self.elmo(character_ids)['elmo_representations'][0].permute(1, 0, 2)\n",
    "        # print(embeded)\n",
    "        \n",
    "        #embeded = self.embed(input_var)\n",
    "        #embeded = self.dropout1(embeded)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeded, input_len)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        # method-1: as it is a classification problem, we just grab the last hidden state -0.85\n",
    "        outputs = F.relu(self.dropout2( torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) ))\n",
    "        \n",
    "        # method-2: cat last 2 hidden state , avg-pooling and max-pooling - 0.86\n",
    "        # avgpool = F.adaptive_avg_pool1d(outputs.permute(1, 2, 0), 1).squeeze(2)\n",
    "        # maxpool = F.adaptive_max_pool1d(outputs.permute(1, 2, 0), 1).squeeze(2)\n",
    "        # outputs =  F.relu(self.dropout2( torch.cat((hidden[-2, :, :], hidden[-1, :, :], avgpool, maxpool), dim=1) ))\n",
    "        \n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers * 2, self.batch_size, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/15/2019 16:03:08 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ELMoGRU(device, options_file, weight_file, batch_size=8)\n",
    "model.to(model.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Iteration : 10 , Time : 8.45 , TrainLoss : 2.7485\n",
      "Epoch : 1 , Iteration : 20 , Time : 6.29 , TrainLoss : 2.8706\n",
      "Epoch : 1 , Iteration : 30 , Time : 7.71 , TrainLoss : 2.8742\n",
      "Epoch : 1 , Iteration : 40 , Time : 7.09 , TrainLoss : 2.8756\n",
      "Epoch : 1 , Iteration : 50 , Time : 6.42 , TrainLoss : 2.8603\n",
      "Epoch : 1 , Iteration : 60 , Time : 7.04 , TrainLoss : 2.8443\n",
      "Epoch : 1 , Iteration : 70 , Time : 8.96 , TrainLoss : 2.8304\n",
      "Epoch : 1 , Iteration : 80 , Time : 8.59 , TrainLoss : 2.8286\n",
      "Epoch : 1 , Iteration : 90 , Time : 10.22 , TrainLoss : 2.8029\n",
      "Epoch : 1 , Iteration : 100 , Time : 7.65 , TrainLoss : 2.7807\n",
      "Epoch : 1 , Iteration : 110 , Time : 6.92 , TrainLoss : 2.7667\n",
      "Epoch : 1 , Iteration : 120 , Time : 5.98 , TrainLoss : 2.7483\n",
      "Epoch : 1 , Iteration : 130 , Time : 8.93 , TrainLoss : 2.7284\n",
      "Epoch : 1 , Iteration : 140 , Time : 7.25 , TrainLoss : 2.7216\n",
      "Epoch : 1 , Iteration : 150 , Time : 6.23 , TrainLoss : 2.7150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-35d90447c42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# here same as optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d6179feefaa4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs, input_len, hidden)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcharacter_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0membeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elmo_representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# print(embeded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, word_inputs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# run the biLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbilm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_word_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mmask_with_bos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, word_inputs)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mtype_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mlstm_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# Prepare the output.  The first layer is duplicated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/elmo_lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mstacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_and_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturned_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_sequence_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/encoder_base.py\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[0;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Actually call the module on the sorted PackedSequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/elmo_lstm.py\u001b[0m in \u001b[0;36m_lstm_forward\u001b[0;34m(self, inputs, initial_state)\u001b[0m\n\u001b[1;32m    215\u001b[0m             forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n\u001b[1;32m    216\u001b[0m                                                                    \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                                                    forward_state)\n\u001b[0m\u001b[1;32m    218\u001b[0m             backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n\u001b[1;32m    219\u001b[0m                                                                       \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/charley/lib/python3.6/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, batch_lengths, initial_state)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 timestep_output = torch.clamp(timestep_output,\n\u001b[1;32m    208\u001b[0m                                               \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_projection_clip_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                               self.state_projection_clip_value)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Only do dropout if the dropout prob is > 0.0 and we are in training mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, epoch, criterion, optimizer, x_train, y_train, x_test, y_test, input_lang, clip=1., batch_sz=256):\n",
    "    n_batches_train = len(x_train) // batch_sz\n",
    "    n_batches_test = len(x_test) // batch_sz\n",
    "    \n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    train_loss = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, batch in enumerate(get_batches(x_train, y_train, batch_size=batch_sz), 1):\n",
    "        bx, bx_len, by = batch\n",
    "        int_seq = [[input_lang.idx2word[idx] for idx in sent] for sent in bx]\n",
    "\n",
    "        by = torch.tensor(by).to(model.device) # tgt without modified - cuda out of memory\n",
    "\n",
    "        optimizer.zero_grad() # here same as optimizer.zero_grad()\n",
    "        hidden = hidden.detach()\n",
    "        outputs, hidden = model(int_seq, bx_len, hidden)\n",
    "\n",
    "        loss = criterion(outputs, by)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i % 5 == 0:\n",
    "            # print loss info every 20 Iterations\n",
    "            log_str = \"Epoch : {} , Iteration : {} , Time : {:.2f} , TrainLoss : {:.4f}\".format \\\n",
    "                        (epoch, i, time.time()-t0, train_loss/i)\n",
    "            print(log_str)\n",
    "            t0 = time.time()\n",
    "\n",
    "    train_loss = train_loss / n_batches_train\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    corr = total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(get_batches(x_test, y_test, batch_size=batch_sz), 1):\n",
    "            bx, bx_len, by = batch\n",
    "            total += len(by)\n",
    "            int_seq = [[input_lang.idx2word[idx] for idx in sent] for sent in bx]\n",
    "\n",
    "            by = torch.tensor(by).to(model.device)\n",
    "            \n",
    "            outputs, _ = model(int_seq, bx_len, hidden)\n",
    "            loss = criterion(outputs, by)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            corr += (pred.data.cpu().numpy() == by.data.cpu().numpy()).sum()\n",
    "\n",
    "        eval_loss = eval_loss / n_batches_test\n",
    "\n",
    "        accuracy = corr / total\n",
    "    return model, train_loss, eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embed): Embedding(12693, 300)\n",
       "  (dropout1): Dropout(p=0.5)\n",
       "  (lstm): LSTM(300, 256, num_layers=2, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lstm_newspaper import SimpleLSTM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_matrix = np.load('EmbeddingMatrix.npy')\n",
    "model = SimpleLSTM(device, embed_matrix, n_layers=2)\n",
    "model.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight \t torch.Size([12693, 300])\n",
      "lstm.weight_ih_l0 \t torch.Size([1024, 300])\n",
      "lstm.weight_hh_l0 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 \t torch.Size([1024])\n",
      "lstm.bias_hh_l0 \t torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse \t torch.Size([1024, 300])\n",
      "lstm.weight_hh_l0_reverse \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse \t torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse \t torch.Size([1024])\n",
      "lstm.weight_ih_l1 \t torch.Size([1024, 512])\n",
      "lstm.weight_hh_l1 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l1 \t torch.Size([1024])\n",
      "lstm.bias_hh_l1 \t torch.Size([1024])\n",
      "lstm.weight_ih_l1_reverse \t torch.Size([1024, 512])\n",
      "lstm.weight_hh_l1_reverse \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l1_reverse \t torch.Size([1024])\n",
      "lstm.bias_hh_l1_reverse \t torch.Size([1024])\n",
      "fc.weight \t torch.Size([20, 512])\n",
      "fc.bias \t torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for para_tensors in model.state_dict():\n",
    "    print(para_tensors, '\\t', model.state_dict()[para_tensors].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charley",
   "language": "python",
   "name": "charley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
