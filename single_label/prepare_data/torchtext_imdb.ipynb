{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext import data\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stoplist = stopwords.words('english')\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'something', 'four', 'give', 'all', 'already', 'amount', 'herein', 'else', 'hereby', 'whole', 'fifteen', 'ten', 'why', 'me', 'eight', 'anyhow', 'even', 'hereafter', 'first', 'everyone', 'these', 'using', 'via', 'whose', 'after', 'seem', 'someone', 'last', 'sometimes', 'too', 'us', 'because', 'next', 'almost', 'any', 'under', 'each', 'ever', 'therefore', 'alone', 'see', 'get', 'either', 'in', 'full', 'across', 'he', 'whence', 'wherein', 'being', 'him', 'less', 'whereby', 'twenty', 'towards', 'doing', 'yourselves', 'one', 'yourself', 'on', 'more', 'elsewhere', 'becomes', 'nothing', 'serious', 'put', 'always', 'six', 'may', 'done', 'themselves', 'whom', 'indeed', 'call', 'a', 'herself', 'hundred', 'noone', 'bottom', 'amongst', 'must', 'formerly', 'around', 'once', 'should', 'show', 'when', 'whereupon', 'back', 'for', 'just', 'regarding', 're', 'somehow', 'thus', 'to', 'can', 'above', 'perhaps', 'anywhere', 'down', 'myself', 'mine', 'himself', 'over', 'together', 'well', 'few', 'i', 'anything', 'ourselves', 'such', 'whither', 'how', 'who', 'except', 'which', 'hereupon', 'my', 'five', 'nobody', 'also', 'between', 'other', 'none', 'ours', 'further', 'thereupon', 'might', 'along', 'during', 'front', 'into', 'its', 'now', 'whoever', 'least', 'we', 'every', 'his', 'you', 'seemed', 'neither', 'became', 'much', 'really', 'keep', 'otherwise', 'nine', 'am', 'name', 'sixty', 'have', 'would', 'could', 'up', 'never', 'namely', 'is', 'fifty', 'itself', 'among', 'hence', 'moreover', 'sometime', 'as', 'thru', 'besides', 'by', 'seeming', 'say', 'what', 'anyone', 'side', 'with', 'do', 'does', 'behind', 'our', 'but', 'latterly', 'move', 'upon', 'thence', 'toward', 'whatever', 'only', 'from', 'quite', 'still', 'without', 'before', 'those', 'anyway', 'an', 'whenever', 'whereafter', 'nevertheless', 'top', 'where', 'some', 'here', 'ca', 'enough', 'unless', 'was', 'if', 'forty', 'throughout', 'this', 'same', 'are', 'mostly', 'although', 'it', 'has', 'others', 'or', 'seems', 'thereafter', 'and', 'though', 'while', 'been', 'eleven', 'various', 'her', 'take', 'whereas', 'beforehand', 'everything', 'thereby', 'both', 'against', 'meanwhile', 'empty', 'off', 'somewhere', 'about', 'she', 'go', 'then', 'were', 'your', 'onto', 'therein', 'cannot', 'through', 'everywhere', 'again', 'beside', 'so', 'former', 'that', 'please', 'third', 'within', 'used', 'below', 'them', 'very', 'another', 'of', 'wherever', 'many', 'three', 'due', 'per', 'at', 'beyond', 'however', 'be', 'often', 'they', 'will', 'own', 'did', 'two', 'twelve', 'made', 'out', 'yours', 'nowhere', 'there', 'becoming', 'yet', 'no', 'rather', 'make', 'several', 'the', 'had', 'latter', 'until', 'not', 'part', 'their', 'nor', 'most', 'whether', 'than', 'afterwards', 'since', 'hers', 'become'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(STOP_WORDS) # <- set of Spacy's default stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(stoplist) & STOP_WORDS\n",
    "# set(stoplist) - STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', lower=True, include_lengths=True, stop_words=STOP_WORDS)\n",
    "# TEXT = data.Field(lower=True)\n",
    "LABEL = data.Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['worst', 'movies', 'production', ',', 'ever.<br', '/><br', '/>1', '.', 'exciting', 'beginning', ',', 'guy', 'walking', '...', 'walking', '...', 'walking', '(', 'spoiler', ')', '.', '15', 'minutes', 'walking', '.', '?', '<', 'br', '/><br', '/>2', '.', 'mention', \"'s\", 'lot', 'issues', 'lighting', ',', \"'s\", 'like', 'shot', 'night', 'scenes', 'day', '.', '<', 'br', '/><br', '/>3', '.', 'acting', 'terrible', '.', 'looks', 'like', 'found', 'community', 'theater', '(', 'mexico', ')', '...', 'took', 'people', 'turned', 'away.<br', '/><br', '/>please', ',', 'love', 'holy', ',', \"n't\", 'rent', 'movie', '.', 'know', 'owns', ',', 'apologize', '.', 'director', 'subject', 'punishment', 'war', 'crimes', 'tribunal', 'foisting', 'public', '.'], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method build_vocab in module torchtext.data.field:\n",
      "\n",
      "build_vocab(*args, **kwargs) method of torchtext.data.field.Field instance\n",
      "    Construct the Vocab object for this field from one or more datasets.\n",
      "    \n",
      "    Arguments:\n",
      "        Positional arguments: Dataset objects or other iterable data\n",
      "            sources from which to construct the Vocab object that\n",
      "            represents the set of possible values for this field. If\n",
      "            a Dataset object is provided, all columns corresponding\n",
      "            to this field are used; individual columns can also be\n",
      "            provided directly.\n",
      "        Remaining keyword arguments: Passed to the constructor of Vocab.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TEXT.build_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "25002\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "print(len(TEXT.vocab))\n",
    "LABEL.build_vocab(train_data)\n",
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['<unk>'], TEXT.vocab.stoi['<pad>'], TEXT.vocab.stoi['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 275277), ('.', 236270), ('\"', 63334), (\"'s\", 62090), ('-', 52864), ('/><br', 50935), ('movie', 43059), ('film', 39280), ('(', 33106), (')', 32848), (\"n't\", 32846), ('!', 21780), ('like', 20111), (\"'\", 17015), ('good', 14903), ('?', 14799), ('time', 12335), ('story', 11718), ('...', 9726), (':', 9388)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', ',', '.', '\"', \"'s\", '-', '/><br', 'movie', 'film']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7fcd77f0db70>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10270"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for word, cc in TEXT.vocab.freqs.most_common():\n",
    "    if cc >= 30:\n",
    "        words.append(word)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10272"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {'<unk>': 0, '<pad>': 1}\n",
    "for i, word in enumerate(words, 2):\n",
    "    word2idx[word] = i\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'neg'\n",
    "1 if a == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = [], []\n",
    "x_test, y_test = [], []\n",
    "\n",
    "for item in train_data:\n",
    "    x_train.append([word2idx.get(word, 0) for word in item.text])\n",
    "    y_train.append(1 if item.label == 'pos' else 0)\n",
    "\n",
    "for item in test_data:\n",
    "    x_test.append([word2idx.get(word, 0) for word in item.text])\n",
    "    y_test.append(1 if item.label == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(y_train) == 25000\n",
    "assert len(x_test) == len(y_test) and len(y_test) == 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = {'train': {'x': x_train, 'y': y_train}, \n",
    "        'test': {'x': x_test, 'y': y_test}, \n",
    "        'word2idx': word2idx}\n",
    "\n",
    "with open('imdb_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52, 203, 3, 0, 9049, 126, 8512, 2598, 5171, 9813, 0, 9049, 797, 3, 0, 1196, 4002, 1794, 0, 4, 1287, 5406, 4, 3, 965, 9, 837, 419, 13], [1008, 510, 2220, 0, 512, 914, 3, 0, 3132, 2, 151, 373, 3, 503, 5011, 7, 71, 12, 122, 1795, 3, 5348, 1222, 797, 1084, 547, 6, 1274, 502, 6, 0, 7127, 0, 852, 2, 3970, 3, 321, 3971, 1399, 10, 33, 0, 11, 6, 14, 132, 12, 3, 500, 700, 2, 872, 55, 0, 1245, 3, 230, 592, 428, 10, 503, 2181, 653, 11, 611, 13, 50, 35, 4, 984, 5829, 4, 182, 132, 2, 4498, 428, 611, 871, 132, 3, 112, 2311, 39, 65, 2679, 3, 115, 112, 45, 0, 7, 51, 352]]\n"
     ]
    }
   ],
   "source": [
    "with open('imdb_data.pkl', 'rb') as f:\n",
    "    new_data = pickle.load(f)\n",
    "    \n",
    "print(new_data['train']['x'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_data['train']['y'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(new_data['train']['y'][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/imdb_data.pkl', 'rb') as rf:\n",
    "    data_split = pickle.load(rf)\n",
    "MAX_LEN = 100\n",
    "x_train, y_train = data_split['train']['x'], data_split['train']['y']\n",
    "x_test, y_test = data_split['test']['x'], data_split['test']['y']\n",
    "x_train = [x[:MAX_LEN] for x in x_train]\n",
    "x_test = [x[:MAX_LEN] for x in x_test]\n",
    "word2idx = data_split['word2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['<pad>'], word2idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52, 203, 3, 0, 9049, 126, 8512, 2598, 5171, 9813, 0, 9049, 797, 3, 0, 1196, 4002, 1794, 0, 4, 1287, 5406, 4, 3, 965, 9, 837, 419, 13], [1008, 510, 2220, 0, 512, 914, 3, 0, 3132, 2, 151, 373, 3, 503, 5011, 7, 71, 12, 122, 1795, 3, 5348, 1222, 797, 1084, 547, 6, 1274, 502, 6, 0, 7127, 0, 852, 2, 3970, 3, 321, 3971, 1399, 10, 33, 0, 11, 6, 14, 132, 12, 3, 500, 700, 2, 872, 55, 0, 1245, 3, 230, 592, 428, 10, 503, 2181, 653, 11, 611, 13, 50, 35, 4, 984, 5829, 4, 182, 132, 2, 4498, 428, 611, 871, 132, 3, 112, 2311, 39, 65, 2679, 3, 115, 112, 45, 0, 7, 51, 352], [353, 7581, 4907, 2, 810, 674, 2, 8216, 984, 668, 972, 2, 1525, 2, 9, 0, 10, 559, 2890, 11, 7457, 0, 6, 776, 0, 2204, 7828, 4409, 3, 393, 1370, 4908, 972, 3615, 0, 272, 1015, 6, 1197, 559, 0, 3, 5, 9, 6, 1117, 42, 206, 16, 1515, 32, 340, 7030, 3268, 2121, 8, 3, 5889, 6736, 668, 0, 3269, 3042, 4108, 10, 6737, 1408, 11, 1097, 96, 622, 532, 4370, 3, 302, 517, 3268, 2040, 95, 1536, 1125, 966, 3, 96, 0, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([57, 810, 353, 4, 1278, 4, 2, 1003, 1912, 1191, 385, 674, 0, 1464, 1967, 1181, 0, 2, 111, 5, 4, 0, 4, 3, 2977, 932, 0, 10, 6, 3253, 11, 0, 838, 6, 1478, 89, 798, 0, 0, 9632, 36, 444, 685, 2, 323, 8951, 72, 4064, 2, 656, 4, 1258, 537, 5647, 14, 4072, 3, 1347, 1239, 0, 2750, 9512, 6, 107, 2542, 14, 97, 180, 18, 10, 117, 25, 2911, 511, 6556, 6, 3304, 11, 2, 448, 973, 212, 6, 2656, 222, 2, 3892, 0, 59, 3828, 0, 3, 387, 2397, 0, 98, 999, 0, 0, 10], [8, 250, 0, 0, 10, 4027, 6, 0, 0, 11, 3, 14, 411, 0, 48, 0, 3, 173, 662, 6195, 0, 2, 4, 14, 0, 26, 27, 7, 71, 148, 14, 1154, 6240, 4, 193, 3075, 4, 111, 3, 2221, 486, 991, 2, 604, 4235, 8, 68, 857, 0, 3, 604, 0, 2323, 2259, 54, 435, 482, 435, 2, 0, 7, 426, 592, 8, 133, 2207, 621, 3, 2, 2, 8, 5, 37, 16, 3, 5, 213, 2, 205, 200, 12, 1109, 3, 125, 205, 200, 601, 197, 16, 19, 2, 606, 360, 41, 3, 78, 12, 2088, 3, 162], [341, 2, 810, 545, 2349, 4146, 3131, 0, 7, 0, 848, 28, 278, 9634, 334, 2508, 3, 0, 979, 3, 979, 6217, 2313, 3, 979, 1678, 272, 44, 1656, 3, 3300, 0, 2, 979, 4004, 7, 1916, 2, 12, 4147, 1018, 0, 2, 0, 2, 1386, 0, 2, 4001, 5389, 189, 2772, 0, 8842, 192, 3158, 3, 9, 14, 2506, 975, 7073, 8611, 485, 6, 6, 1415, 0, 1177, 3078, 36, 1121, 0, 2226, 3369, 3, 189, 7841, 60, 205, 3, 2113, 205, 6, 0, 0, 0, 1202, 0, 0, 5425, 3, 4146, 3131, 3, 1386, 8, 3, 3145, 0], [989, 4, 5107, 38, 4, 28, 197, 17, 2240, 0, 7, 0, 1993, 7575, 345, 6, 5227, 400, 59, 605, 0, 0, 506, 189, 3, 212, 607, 3, 212, 9, 633, 7273, 164, 0, 1337, 132, 1300, 4, 4848, 4, 189, 1465, 10259, 7, 228, 5, 254, 4, 52, 4, 167, 4282, 211, 4, 464, 244, 4, 2, 0, 4282, 211, 1070, 20, 319, 7, 228, 5, 4, 4848, 4, 145, 5, 266, 18, 1304, 639, 7, 228, 5, 4, 2483, 4, 2, 145, 5, 260, 202, 3696, 515, 14, 1310, 2290, 6394, 20, 319, 7, 597, 2, 2403, 3], [133, 4405, 3, 133, 40, 358, 3, 133, 40, 2218, 3, 0, 1182, 164, 8, 548, 3, 127, 0, 133, 346, 3079, 4351, 7, 71, 1027, 127, 1982, 131, 65, 5, 67, 8, 3, 358, 73, 4095, 2, 196, 0, 0, 126, 4313, 3, 594, 65, 251, 682, 545, 47, 295, 441, 1001, 105, 119, 682, 3, 8, 191, 808, 189, 2489, 40, 0, 7, 1081, 5856, 3, 12, 33, 8, 74, 33, 181, 8, 3], [2, 3487, 168, 3, 1194, 1942, 1297, 411, 538, 5, 3689, 3390, 3, 119, 2593, 931, 3, 2, 8, 1506, 16, 731, 274, 3, 64, 75, 1436, 1178, 4, 543, 846, 1456, 4, 592, 2, 2393, 12, 45, 168, 3, 168, 76, 1677, 3], [16, 9, 3, 12, 158, 1195, 3386, 568, 41, 2, 101, 2, 1155, 3, 6409, 4432, 582, 1562, 0, 3, 3621, 3, 58, 120, 346, 1439, 3, 552, 9, 3], [8, 4626, 284, 47, 2, 3313, 309, 2, 3313, 5399, 2084, 10, 243, 32, 11, 2, 5, 42, 103, 6, 16, 28, 50, 35, 3, 2240, 143, 344, 3])\n",
      "[100 100 100 100  76  44  30  29]\n",
      "(0, 0, 0, 0, 0, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# def collate_fn(insts, PAD_token=1):\n",
    "#     # if seq_pad in class then all seqs with same length\n",
    "#     maxlen = max([len(x) for x in insts])\n",
    "#     #maxlen = 24\n",
    "#     seq = np.array([x + [PAD_token] * (maxlen - len(x)) for x in insts])\n",
    "#     seq_lens = np.array([len(x) for x in insts])\n",
    "#     return torch.LongTensor(seq), torch.LongTensor(seq_lens)\n",
    "\n",
    "def paired_collate_fn(insts):\n",
    "    #src_insts, tgt_insts = list(zip(*insts))\n",
    "    seq_pairs = sorted(insts, key=lambda p: len(p[0]), reverse=True)\n",
    "    src_insts, tgt_insts = zip(*seq_pairs)\n",
    "    src_len = np.array([len(x) for x in src_insts])\n",
    "    # tgt_insts = collate_fn(tgt_insts)\n",
    "    return (src_insts, src_len, tgt_insts)\n",
    "\n",
    "class IMDBdatasets(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        # self.device = device\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                        IMDBdatasets(x_train, y_train),\n",
    "                        num_workers = 2,\n",
    "                        batch_size = 8,\n",
    "                        collate_fn = paired_collate_fn,\n",
    "                        shuffle = True,\n",
    "                        drop_last = True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src, src_lens, tgt = batch\n",
    "    print(src)\n",
    "    print(src_lens)\n",
    "    print(tgt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 144.62662, 110.63362783247958, 1624)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lengths = [len(ss) for ss in (new_data['train']['x'] + new_data['test']['x'])]\n",
    "\n",
    "np.min(lengths), np.mean(lengths), np.std(lengths), np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9789 words with pre-trained vector in vocab_size = 10272.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/data/charley/crawl-300d-2M.pkl', 'rb') as pf:\n",
    "    fb_w2v = pickle.load(pf)\n",
    "    \n",
    "embed_dim = 300\n",
    "embed_matrix = np.zeros((len(word2idx), embed_dim))\n",
    "flag = 0\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        word_vec = fb_w2v[word]\n",
    "    except:\n",
    "        word_vec = None\n",
    "    if word_vec is not None:\n",
    "        embed_matrix[idx] = word_vec\n",
    "        flag += 1\n",
    "print(\"There are {} words with pre-trained vector in vocab_size = {}.\".format(flag, len(word2idx)))\n",
    "np.save('imdb_EmbeddingMatrix', embed_matrix) # saved as EmbeddingMatrix.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iters, test_iters = data.BucketIterator.splits((train_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5945,    10,    12,  ...,    10,     5,   669],\n",
      "        [    6,     7,    90,  ...,    20,   161,    10],\n",
      "        [ 1106,    30,    26,  ...,     7,     8,    14],\n",
      "        ...,\n",
      "        [   87,     1,     1,  ...,     1,     1,     1],\n",
      "        [   72,     1,     1,  ...,     1,     1,     1],\n",
      "        [16698,     1,     1,  ...,     1,     1,     1]], device='cuda:1')\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iters))\n",
    "print(batch.text) # 527 x 8 = seq_len x B\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_data.examples:\n",
    "    label = vars(x)['label'][0]\n",
    "    if label == '<unk>':\n",
    "        print(vars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_data.examples:\n",
    "    if (x.label != 'pos' and x.label != 'neg'):\n",
    "        print(x.text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charley",
   "language": "python",
   "name": "charley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
